{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoiceID: Customer verification with voice\n",
    "\n",
    "Author: Dolgor Purbueva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "The goal of this project is to develop a voice-based client verification system for a digital bank, leveraging machine learning to enhance security and client authentication process. The system will verify clients by identifying PIN from speech and analyzing their unique vocal patterns during authentication with PIN that was prerecorded during enrollment.\n",
    "\n",
    "The project will utilize CNN machine learning algorithm to classify whether a PIN is correct and a given voice belongs to a registered client or not. Key features of the dataset include audio recordings, which will be converted into waveforms for processing. The primary metric for evaluating the effectiveness of the verification system will be accuracy. We will also take into account precision, recall, and F1-score, which will help in assessing the models performance.\n",
    "\n",
    "The development process will involve data preprocessing, feature engineering, exploratory data analysis, and the implementation of a Convolutional Neural Network (CNN) to classify the voices. The outcome of this project is expected to provide a secure client verification system based on voice that enhances the security of the digital bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "Voice authentication, or voice biometrics, is rapidly becoming one of the most advanced and secure methods of user authentication. In the digital banking world, where security breaches and fraud can have significant financial and reputational impacts, voice authentication offers a powerful solution. By leveraging the unique characteristics of an individual’s voice, this technology adds a strong layer of protection, reducing the risks associated with traditional methods like passwords or PINs.\n",
    "\n",
    "For instance, [HSBC UK saw a 50% drop in telephone fraud after implementing voice biometrics](https://www.about.hsbc.co.uk/news-and-media/hsbc-uks-voice-id-prevents-gbp249-million-of-attempted-fraud), highlighting the tangible benefits of this technology in reducing fraudulent activities. Additionally, adopting such advanced security measures can differentiate companies in competitive markets, further strengthening their brand reputation and customer trust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stakeholder\n",
    "\n",
    "The primary stakeholder for this project is [Revolut](https://en.wikipedia.org/wiki/Revolut), a global neobank and financial technology company headquartered in London, UK. Revolut offers a wide range of banking services for both retail customers and businesses, all through its online platform. Unlike traditional banks, Revolut operates entirely through digital channels, without physical branch networks.\n",
    "\n",
    "Currently, Revolut clients use a combination of PINs, passcodes, and biometric methods (such as fingerprint or facial recognition) for authentication in the mobile app. When logging into the web app, Revolut employs two-factor authentication (2FA) through push notifications or SMS.\n",
    "\n",
    "For an online bank like Revolut, ensuring the security of client accounts is paramount. As there are no physical branches for face-to-face verification, digital security measures must be highly robust to protect against fraud and unauthorized access. With the increasing sophistication of cyberattacks, enhancing security through voice-based client verification adds an extra layer of protection. This method is not only user-friendly but also difficult to replicate, making it a valuable tool in safeguarding clients' financial information. By adopting voice verification, Revolut can strengthen its security infrastructure, improve customer trust, and reduce the risk of fraud.\n",
    "\n",
    "\n",
    "![Alt text](images/image2.png)\n",
    "\n",
    "## Why Voice Authentication is a Good Idea for Revolut\n",
    "\n",
    "* Enhanced Security and Fraud Prevention: Voice authentication uses unique biometric markers, making it harder to replicate than PINs or passwords.\n",
    "* Improved Customer Trust and Satisfaction: Voice authentication is quick and convenient, improving user experience.\n",
    "* Operational Efficiency and Cost Savings: Reducing reliance on traditional security methods can lower fraud investigation costs and streamline authentication.\n",
    "\n",
    "## Business Questions\n",
    "* How can Revolut enhance their online security for clients using machine learning?\n",
    "* What machine learning models can be used and most effective for client verification with voice tasks?\n",
    "\n",
    "## Objectives\n",
    "* Develop classification model for voice-based client verification.\n",
    "* Achieve high prediction accuracy in verifying clients.\n",
    "* Compare and evaluate the performance of different algorithms.\n",
    "* Provide actionable business recommendations to the stakeholder based on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current voice verification technology\n",
    "\n",
    "Current voice verification technology leverages advanced machine learning models, particularly deep learning architectures like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), to analyze and authenticate a user’s identity based on their voice. These systems extract unique voice features, such as pitch, tone, and speech patterns, from audio inputs, often using representations like Mel spectrograms or raw waveforms. Challenges include handling background noise, voice variations due to health or mood, and ensuring data privacy remain key areas of ongoing research and development.\n",
    "\n",
    "Voice verification technology can be broadly categorized into the following types:\n",
    "\n",
    "* Text-Dependent Voice Verification:\n",
    "    * Fixed Phrases: The user must repeat a specific phrase or set of words during verification. The system compares the spoken phrase with a previously recorded version to verify identity.\n",
    "    * Prompted Phrases: The system randomly prompts the user to say a different phrase each time, making it harder for potential impostors to mimic. \n",
    "* Text-Independent Voice Verification: In this type, the system verifies the user's identity regardless of what they say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding\n",
    "For this project, I used the **Speech Commands Dataset v0.02.** This dataset consists of one-second .wav audio files, each containing a single spoken English word. The words belong to a small set of commands, and the audio files are spoken by various speakers. The dataset is organized into folders based on the specific word contained in each audio file. \n",
    "\n",
    "The [dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) is available as part of the TensorFlow Datasets catalog.\n",
    "\n",
    "* The dataset contains over 105,000 one-second audio samples.\n",
    "* It includes 35 unique spoken words.\n",
    "* In addition to the main command words, there are background noise samples and a few additional labels like \"silence\" and \"unknown\" to help with noise handling.\n",
    "\n",
    "**Citations:**\n",
    "\n",
    "```\n",
    "@article{speechcommandsv2,\n",
    "   author = {{Warden}, P.},\n",
    "    title = \"{Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition}\",\n",
    "  journal = {ArXiv e-prints},\n",
    "archivePrefix = \"arXiv\",\n",
    "   eprint = {1804.03209},\n",
    " primaryClass = \"cs.CL\",\n",
    " keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},\n",
    "     year = 2018,\n",
    "    month = apr,\n",
    "    url = {https://arxiv.org/abs/1804.03209},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"/root/fireworks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model Creation/Evaluation Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Audio Data Manipulation Imports\n",
    "import random\n",
    "import IPython.display as ipd\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "# File Path Navigation Import\n",
    "import os\n",
    "\n",
    "# Model Saving Import\n",
    "import pickle\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We have 105,000 one-second audio files across 35 different folders. Let's try to process those files into some form of data frame.  \n",
    "\n",
    "First we will list all the files and their folders to see how many of each we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make a df from folders\n",
    "def list_files_with_subfolders(base_path):\n",
    "    files_list = []\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file_name in files:\n",
    "            subfolder_name = os.path.relpath(root, base_path)\n",
    "            files_list.append({'Folder': subfolder_name, 'File': file_name})\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the folder with data set\n",
    "base_directory = '/root/learning-nn/resources/speech_commands'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the function and save into df\n",
    "files_list = list_files_with_subfolders(base_directory)\n",
    "files_df = pd.DataFrame(files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our data frame. 105841 rows × 2 columns.\n",
    "File coumn contains the name of the file and Folder column is the name of the folder where file is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how many files are in each folder. We see that some files are stored outside of folders and thus have '.' value in the column. Those are administartive files, like readme, license etc. We dont need them for analysis and will drop them from the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df.value_counts('Folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnecessary files\n",
    "files_df = files_df[files_df['Folder'] != '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets extract User ID from file names into a separate column which we will use for our model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore copy warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#extractin used id\n",
    "files_df['UserID'] = files_df['File'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking df with a new column\n",
    "files_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how many recording did users provide to the data set. Majority of users provided 25-50 recordings. Some user provided around 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting files per user\n",
    "user_file_counts = files_df['UserID'].value_counts().reset_index()\n",
    "user_file_counts.columns = ['UserID', 'FileCount']\n",
    "user_file_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting counts by users\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(user_file_counts['UserID'], user_file_counts['FileCount'], color='skyblue')\n",
    "plt.xlabel('UserID')\n",
    "plt.ylabel('Number of Files')\n",
    "plt.title('Number of Files by UserID')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting counts by users\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(user_file_counts['FileCount'], bins=100, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Number of Files')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Distribution of Number of Files per User')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see at the counts of files in each folder. Words like digits, yes, no, go, stop have from 3500 to 4000 files. The rest are below 2500 counts. We might want to use words with hugher counts so our model has more data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_counts = files_df['Folder'].value_counts().reset_index()\n",
    "folder_counts.columns = ['Folder', 'FolderCount']\n",
    "folder_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting files by folder\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(folder_counts['Folder'], folder_counts['FolderCount'])\n",
    "plt.xlabel('Folder')\n",
    "plt.ylabel('Number of Files')\n",
    "plt.title('Number of Files by Folder')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a look at a WAV file\n",
    "\n",
    "A WAV (Waveform Audio File Format) file is a standard audio file format used to store audio data.\n",
    "\n",
    "We will take one random file and explore what characteristics it has and how we can use it for future modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick one file\n",
    "file_path = 'data_raw/backward/0a2b400e_nohash_1.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the .wav file\n",
    "waveform, sample_rate = torchaudio.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print size, sample rate\n",
    "print(waveform)\n",
    "print(waveform.shape)\n",
    "print(sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the WAV file using the wave module\n",
    "with wave.open(file_path, 'r') as wav_file:\n",
    "    # Extract basic metadata\n",
    "    num_channels = wav_file.getnchannels()  # Number of audio channels\n",
    "    sample_width = wav_file.getsampwidth()  # Bit depth (in bytes)\n",
    "    sample_rate = wav_file.getframerate()   # Sample rate (samples per second)\n",
    "    num_frames = wav_file.getnframes()      # Total number of frames (samples per channel)\n",
    "    duration = num_frames / float(sample_rate)  # Duration of the audio in seconds\n",
    "\n",
    "# Convert bit depth from bytes to bits\n",
    "bit_depth = sample_width * 8\n",
    "\n",
    "# Get the file size in bytes\n",
    "file_size = os.path.getsize(file_path)\n",
    "\n",
    "# Print the extracted metadata\n",
    "print(f\"File: {file_path}\")\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "print(f\"Bit Depth: {bit_depth} bits\")\n",
    "print(f\"Number of Channels: {num_channels}\")\n",
    "print(f\"Duration: {duration:.2f} seconds\")\n",
    "print(f\"File Size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_np = waveform.squeeze().numpy()\n",
    "\n",
    "# Plot the waveform using waveshow\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(waveform_np)\n",
    "plt.title('Waveform')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram_transform = T.MelSpectrogram(sample_rate=8000, n_mels=128)\n",
    "mel_spectrogram = mel_spectrogram_transform(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram_np = mel_spectrogram.squeeze().numpy()\n",
    "\n",
    "# Plot the Mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram_np, aspect='auto', origin='lower')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "For our project we had to decide on choice of input: **Waveform vs. Mel Spectrogram.**\n",
    "\n",
    "After reseacrh, we decided to use the raw waveform of the audio signals as the input representation instead of converting the audio to Mel spectrograms. Key reasons for this choice:\n",
    "* Preserves Raw Data: Using the raw waveform keeps the original audio data intact, avoiding potential loss of information during conversion to a spectrogram.\n",
    "* Simplicity: Raw waveforms require fewer preprocessing steps, reducing the complexity of the pipeline.\n",
    "* Model Flexibility: Some modern neural networks can effectively learn features directly from raw waveforms, potentially capturing more nuanced patterns.\n",
    "* Adaptability: Raw waveforms allow the model to adapt to various audio characteristics without being constrained by predefined spectrogram features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "We chose a Convolutional Neural Network (CNN) for this voice verification project because of its ability to process and extract features directly from audio data. CNNs are effective at detecting patterns in waveforms, like pitch and frequency variations, which are key for distinguishing different voices, making them suitable for voice verification.\n",
    "\n",
    "Overall, CNNs simplify feature extraction and improve accuracy without needing extensive manual work.\n",
    "\n",
    "## Tools\n",
    "\n",
    "We chose PyTorch for building and training our Convolutional Neural Network (CNN) because of several key reasons:\n",
    "\n",
    "* Flexibility: PyTorch allows for easy experimentation with different model architectures, thanks to its dynamic computation graphs.\n",
    "* Ease of Use: Its intuitive interface and clear syntax made implementing complex models simpler and faster.\n",
    "* Python Integration: PyTorch integrates smoothly with Python libraries for data manipulation and visualization.\n",
    "\n",
    "## System acrhitecture \n",
    "\n",
    "The system architecture for this voice-based client verification system consists of two main components:\n",
    "\n",
    "* PIN Verification Module: This module processes the client's spoken 4-digit PIN. A Convolutional Neural Network (CNN) analyzes the audio waveform of the pronounced PIN and compares it to the correct PIN. The output is a binary decision (correct or incorrect).  \n",
    "\n",
    "* Voice ID Verification Module: If the PIN is correct, the system proceeds to verify the client's identity using their voice. Another CNN analyzes the client's voice features and compares them to the stored voice profile. The system outputs a binary decision (client or non-client) based on the match.\n",
    "\n",
    "## Target varible\n",
    "\n",
    "In this project the target variable is a binary label indicating whether the input voice belongs to a client or not:\n",
    "* 1 (or True): If the input voice is verified as belonging to a client.  \n",
    "* 0 (or False): If the input voice does not belong to a client.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation of the voice authentication system will focus on measuring the accuracy of both the PIN verification and voice ID verification steps. Accuracy will be the primary metric, reflecting how well the model correctly identifies valid clients and rejects non-clients. \n",
    "\n",
    "Additionally, for the voice ID verification step, precision, recall, and the F1-score will be used to assess the balance between false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Verifying PIN with Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture\n",
    "        audio > 5 cnn blocks > 4 FC layers > [10]\n",
    "\n",
    "        1. audio [B, 1, 8000] > \n",
    "        2. CNN1: audio [B, 16, 8000] (params in: 1, out: 16 ; stride = 1, kernel = 3, padding = 1)\n",
    "        3. CNN2: audio [B, 32, 4000] > (params in: 16, out: 32 ; stride = 2, kernel = 3, padding = 1)\n",
    "        4. CNN3: audio [B. 64, 2000] > (params in: 32, out: 64 ; stride = 2, kernel = 3, padding = 1)\n",
    "        5. CNN4: audio [B. 128, 1000] > (params in: 64, out: 128 ; stride = 2, kernel = 3, padding = 1)\n",
    "        6. CNN5: audio [B, 256, 500] (params in: 128, out: 256 ; stride = 2, kernel = 3, padding = 1)\n",
    "        7. Flattening to [B, 256 * 500] \n",
    "        8. FC1: audio [B, 256] (params in: 256 * 500, out: 256)\n",
    "        AF: ReLu, Drop out = nn.Dropout(p=0.5)\n",
    "        9. FC2: audio [B, 128] (params in: 256, out: 128)\n",
    "        AF: ReLu, Drop out = nn.Dropout(p=0.5)\n",
    "        10. FC3: audio [B, 64] (params in: 128, out: 64)\n",
    "        AF: ReLu, Drop out = nn.Dropout(p=0.5)\n",
    "        11. FC4: audio [B, 10] (params in: 64, out: 10)\n",
    "        \n",
    "        \n",
    " CNN Block1:\n",
    "    [B, 1, 8000]\n",
    "    1. CNN [B, 16, 8000] (params in: 1, out: 16 ; stride = 1, kernel = 3, padding = 1)\n",
    "    2. BatchNorm layer [B, 16, 8000] - number of features (params channels: 16)\n",
    "        ReLu\n",
    "    3. CNN [B, 16, 8000] (params in: 16, out: 16 ; stride = 1, kernel = 3, padding = 1)\n",
    "    4. BatchNorm layer [B, 16, 8000] - number of features (params channels: 16)\n",
    "        ReLu\n",
    "    5. Residual Connection - [B, 16, 8000] + shortcut([B, 1, 8000])\n",
    "        - shortcut (CNN 1-16, stride = 1) + batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_folder(folder_path, label, limit, target_sr=8000):\n",
    "    # Initialize lists to store audio samples and labels\n",
    "    audio_samples = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            # Load the audio file using torchaudio\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            waveform, original_sample_rate = torchaudio.load(\n",
    "                file_path\n",
    "            )  # loads audio data in waveform and sample rate\n",
    "            resampler = T.Resample(\n",
    "                orig_freq=original_sample_rate, new_freq=target_sr\n",
    "            )  # used to convert all sample rates from orig to 8000\n",
    "            waveform_resampled = resampler(waveform)  # standardize all to 8000\n",
    "\n",
    "            # Ensure waveform is exactly 8000 samples long\n",
    "            if waveform_resampled.shape[1] > 8000:\n",
    "                waveform_resampled = waveform_resampled[\n",
    "                    :, :8000\n",
    "                ]  # change to 8000 again if not\n",
    "            elif waveform_resampled.shape[1] < 8000:\n",
    "                padding_size = 8000 - waveform_resampled.shape[1]\n",
    "                waveform_resampled = F.pad(\n",
    "                    waveform_resampled, (0, padding_size)\n",
    "                )  # if its shorter it fills with 0 to reach 8000\n",
    "\n",
    "            # Add the tensor to the list of samples\n",
    "            audio_samples.append(waveform_resampled)\n",
    "\n",
    "            # Add the corresponding label\n",
    "            # Convert the label to a one-hot encoded tensor\n",
    "            probabilities = [0.0 for i in range(10)]\n",
    "            probabilities[label] = 1.0\n",
    "            label_tensor = torch.tensor(\n",
    "                probabilities, dtype=torch.float32\n",
    "            )  # Ensure correct tensor creation\n",
    "\n",
    "            # Add the one-hot encoded label tensor to the list\n",
    "            labels.append(label_tensor)\n",
    "\n",
    "            if len(audio_samples) == limit:\n",
    "                break\n",
    "\n",
    "    # Stack the samples and labels into tensors\n",
    "    audio_samples_tensor = torch.stack(\n",
    "        audio_samples\n",
    "    )  # Shape: (n_samples, n_mel_bins, n_time_steps)\n",
    "    labels_tensor = torch.stack(labels)  # Shape: (n_samples,)\n",
    "\n",
    "    return audio_samples_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(path, limit):\n",
    "    digit_limit = limit // 10\n",
    "    names = [\n",
    "        \"zero\",\n",
    "        \"one\",\n",
    "        \"two\",\n",
    "        \"three\",\n",
    "        \"four\",\n",
    "        \"five\",\n",
    "        \"six\",\n",
    "        \"seven\",\n",
    "        \"eight\",\n",
    "        \"nine\",\n",
    "    ]\n",
    "\n",
    "    audio_samples_tensors = []\n",
    "    labels_tensors = []\n",
    "\n",
    "    for index, name in enumerate(names):\n",
    "        audio_samples_tensor, labels_tensor = process_audio_folder(\n",
    "            f\"{path}/{name}\", label=index, limit=digit_limit\n",
    "        )\n",
    "        audio_samples_tensors.append(audio_samples_tensor)\n",
    "        labels_tensors.append(labels_tensor)\n",
    "\n",
    "    audio_samples_tensor = torch.cat(audio_samples_tensors, dim=0)\n",
    "    labels_tensor = torch.cat(labels_tensors, dim=0)\n",
    "    return audio_samples_tensor, labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumbersDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio, label):\n",
    "        assert audio.size(0) == label.size(0)\n",
    "        self.audio = audio\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        return (self.audio[index], self.label[index]) #tuples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.audio.size(dim=0) #assuming audio and label tensors are same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "        Architecture\n",
    "        audio > 5 cnn blocks > 4 FC layers > [10]\n",
    "\n",
    "        1. audio [B, 1, 8000] > \n",
    "        2. CNN1: audio [B, 16, 8000] (params in: 1, out: 16 ; stride = 1, kernel = 3, padding = 1)\n",
    "        3. CNN2: audio [B, 32, 4000] > (params in: 16, out: 32 ; stride = 2, kernel = 3, padding = 1)\n",
    "        4. CNN3: audio [B. 64, 2000] > (params in: 32, out: 64 ; stride = 2, kernel = 3, padding = 1)\n",
    "        5. CNN4: audio [B. 128, 1000] > (params in: 64, out: 128 ; stride = 2, kernel = 3, padding = 1)\n",
    "        6. CNN5: audio [B, 256, 500] (params in: 128, out: 256 ; stride = 2, kernel = 3, padding = 1)\n",
    "        7. Flattening to [B, 256 * 500] \n",
    "        8. FC1: audio [B, 256] (params in: 256 * 500, out: 256)\n",
    "        AF: ReLu, Drop out = nn.Dropout(p=0.5)\n",
    "        9. FC2: audio [B, 128] (params in: 256, out: 128)\n",
    "        AF: ReLu, Drop out = nn.Dropout(p=0.5)\n",
    "        10. FC3: audio [B, 64] (params in: 128, out: 64)\n",
    "        AF: ReLu, Drop out = nn.Dropout(p=0.5)\n",
    "        11. FC4: audio [B, 10] (params in: 64, out: 10)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding): \n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv1d(in_channels ,out_channels, kernel_size, stride, padding)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.cnn2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.shortcut_cnn = nn.Conv1d(in_channels ,out_channels, kernel_size, stride, padding)\n",
    "        self.shortcut_batchnorm = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, audio):\n",
    "        x = self.cnn1(audio)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = x + self.shortcut_batchnorm(self.shortcut_cnn(audio))\n",
    "        x = self.relu2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumbersModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # [B, 1, 8000]\n",
    "        self.cnn1 = BlockCNN(in_channels=1, out_channels=16, stride = 1, kernel_size= 3, padding = 1)\n",
    "        # [B, 16, 8000]\n",
    "        self.cnn2 = BlockCNN(in_channels=16, out_channels=32, stride = 2, kernel_size = 3, padding = 1)\n",
    "        # [B, 32, 4000]\n",
    "        self.cnn3 = BlockCNN(in_channels=32, out_channels=64, stride = 2, kernel_size = 3, padding = 1)\n",
    "        # [B, 64, 2000]\n",
    "        self.cnn4 = BlockCNN(in_channels=64, out_channels=128, stride = 2, kernel_size = 3, padding = 1)\n",
    "        # [B, 128, 1000]\n",
    "        self.cnn5 = BlockCNN(in_channels=128, out_channels=256, stride = 2, kernel_size = 3, padding = 1)\n",
    "        # [B, 256, 500]\n",
    "\n",
    "        self.fc1 =  nn.Linear(in_features=256 * 500, out_features=256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout1d(p=0.5)\n",
    "\n",
    "        self.fc2 =  nn.Linear(in_features=256, out_features=128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout1d(p=0.5)\n",
    "\n",
    "        self.fc3 =  nn.Linear(in_features=128, out_features=64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout1d(p=0.5)\n",
    "\n",
    "        self.fc4 =  nn.Linear(in_features=64, out_features=10)\n",
    "    \n",
    "    def forward(self, audio):\n",
    "        x = self.cnn1(audio)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "\n",
    "        x = torch.reshape(x, (x.size(0), x.size(1) * x.size(2)))\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    CNN Block1:\n",
    "    [B, 1, 8000]\n",
    "    1. CNN [B, 16, 8000] (params in: 1, out: 16 ; stride = 1, kernel = 3, padding = 1)\n",
    "    2. BatchNorm layer [B, 16, 8000] - number of features (params channels: 16)\n",
    "        ReLu\n",
    "    3. CNN [B, 16, 8000] (params in: 16, out: 16 ; stride = 1, kernel = 3, padding = 1)\n",
    "    4. BatchNorm layer [B, 16, 8000] - number of features (params channels: 16)\n",
    "        ReLu\n",
    "    5. Residual Connection - [B, 16, 8000] + shortcut([B, 1, 8000])\n",
    "        - shortcut (CNN 1-16, stride = 1) + batchnorm\n",
    "\n",
    "    CNN Block2:\n",
    "    [B, 16, 8000]\n",
    "    1. CNN [B, 32, 4000] (params in: 16, out: 32 ; stride = 2, kernel = 3, padding = 1)\n",
    "    2. BatchNorm layer [B, 32, 4000] - number of features (params channels: 32)\n",
    "        ReLu\n",
    "    3. CNN [B, 32, 4000] (params in: 32, out: 32 ; stride = 1, kernel = 3, padding = 1)\n",
    "    4. BatchNorm layer [B, 32, 4000] - number of features (params channels: 32)\n",
    "        ReLu\n",
    "    5. Residual Connection - [B, 32, 4000] + shortcut([B, 16, 8000])\n",
    "        - shortcut (CNN 16-32, stride = 2) + batchnorm\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions_distribution, labels_distribution):\n",
    "    # predictions_distribution: [B, 10] # class per digit\n",
    "    # labels_distribution: [B, 10] # class per digit\n",
    "\n",
    "    # Convert probabilities to predicted class labels\n",
    "    predictions = torch.argmax(predictions_distribution, dim=1)\n",
    "    labels = torch.argmax(labels_distribution, dim=1)\n",
    "\n",
    "    # Convert to numpy arrays for sklearn functions\n",
    "    predictions_np = predictions.cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(labels_np, predictions_np)\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=50):\n",
    "    model.train() #set model into training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for audio, label in train_loader:\n",
    "            # audio shape is [B, 8000]\n",
    "            audio = audio.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "          \n",
    "            optimizer.zero_grad() #reset optimizer\n",
    "            prediction = model(audio)\n",
    "            loss = criterion(prediction, label)\n",
    "            loss.backward() #calculating updates (derivatives) for weights and biases based on loss\n",
    "            optimizer.step() #use updates and apply to the model\n",
    "            running_loss += loss * audio.size(dim=0)\n",
    "\n",
    "        # calculate average loss for epoch\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        labels = []\n",
    "\n",
    "        running_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for audio, label in test_loader:\n",
    "                # audio shape is [B, 8000]\n",
    "                audio = audio.to(DEVICE)\n",
    "                label = label.to(DEVICE)\n",
    "            \n",
    "                prediction = model(audio) # [B, 10]\n",
    "                loss = criterion(prediction, label) \n",
    "    \n",
    "                running_loss += loss * audio.size(dim=0)\n",
    "\n",
    "                prediction_distribution = F.softmax(prediction, dim=1)\n",
    "                predictions.append(prediction_distribution)\n",
    "                labels.append(label)\n",
    "            \n",
    "\n",
    "        # calculate average loss for epoch\n",
    "        test_loss = running_loss / len(test_loader.dataset)\n",
    "\n",
    "        predictions = torch.cat(predictions)  # put all predictions into a single tensor\n",
    "        labels = torch.cat(labels)  # put all true labels into a single tensor\n",
    "        accuracy = evaluate(predictions, labels)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(dataset_limit: int, batch_size: int, lr: int):\n",
    "    # Prepare data\n",
    "    print(\"Preparing data\")\n",
    "    # set a limit on the total number of pairs to be generated\n",
    "    # one = read_as_dict(\"/root/learning-nn/resources/speech_commands/one\", 1000)\n",
    "\n",
    "    # load data into a dict from marvin folder\n",
    "    audio_samples_tensor, labels_tensor = to_pairs(\n",
    "        \"/root/learning-nn/resources/speech_commands\", dataset_limit\n",
    "    )\n",
    "    print(f\"Read {len(audio_samples_tensor)} pairs\")\n",
    "\n",
    "    as_train, as_test, l_train, l_test = train_test_split(\n",
    "        audio_samples_tensor, labels_tensor, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Setting up the training\n",
    "    print(\n",
    "        f\"Running training. dataset_limit={dataset_limit}. batch_size={batch_size}. lr={lr}\"\n",
    "    )\n",
    "    model = NumbersModel().to(DEVICE)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=NumbersDataset(as_train, l_train), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=NumbersDataset(as_test, l_test), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # criterion = nn.BCELoss()\n",
    "    criterion = nn.BCEWithLogitsLoss() #log loss, criterion for assesing classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=50)\n",
    "\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data\n",
      "Read 20000 pairs\n",
      "Running training. dataset_limit=20000. batch_size=32. lr=0.0005\n",
      "Epoch: 1/50, Train Loss: 0.5336, Test Loss: 0.4667, Accuracy: 0.1240\n",
      "Epoch: 2/50, Train Loss: 0.4454, Test Loss: 0.4028, Accuracy: 0.2980\n",
      "Epoch: 3/50, Train Loss: 0.4152, Test Loss: 0.3788, Accuracy: 0.3807\n",
      "Epoch: 4/50, Train Loss: 0.3898, Test Loss: 0.3551, Accuracy: 0.3982\n",
      "Epoch: 5/50, Train Loss: 0.3666, Test Loss: 0.3096, Accuracy: 0.5413\n",
      "Epoch: 6/50, Train Loss: 0.3507, Test Loss: 0.2993, Accuracy: 0.6152\n",
      "Epoch: 7/50, Train Loss: 0.3365, Test Loss: 0.2920, Accuracy: 0.6228\n",
      "Epoch: 8/50, Train Loss: 0.3260, Test Loss: 0.2808, Accuracy: 0.5142\n",
      "Epoch: 9/50, Train Loss: 0.3166, Test Loss: 0.2710, Accuracy: 0.6040\n",
      "Epoch: 10/50, Train Loss: 0.3113, Test Loss: 0.2588, Accuracy: 0.7133\n",
      "Epoch: 11/50, Train Loss: 0.3066, Test Loss: 0.2431, Accuracy: 0.7765\n",
      "Epoch: 12/50, Train Loss: 0.3037, Test Loss: 0.2480, Accuracy: 0.7722\n",
      "Epoch: 13/50, Train Loss: 0.2990, Test Loss: 0.2371, Accuracy: 0.7738\n",
      "Epoch: 14/50, Train Loss: 0.2980, Test Loss: 0.2379, Accuracy: 0.7925\n",
      "Epoch: 15/50, Train Loss: 0.2976, Test Loss: 0.2311, Accuracy: 0.7987\n",
      "Epoch: 16/50, Train Loss: 0.2959, Test Loss: 0.2286, Accuracy: 0.8050\n",
      "Epoch: 17/50, Train Loss: 0.2957, Test Loss: 0.2322, Accuracy: 0.8090\n",
      "Epoch: 18/50, Train Loss: 0.2939, Test Loss: 0.2241, Accuracy: 0.8167\n",
      "Epoch: 19/50, Train Loss: 0.2938, Test Loss: 0.2174, Accuracy: 0.8135\n",
      "Epoch: 20/50, Train Loss: 0.2950, Test Loss: 0.2335, Accuracy: 0.7720\n",
      "Epoch: 21/50, Train Loss: 0.2923, Test Loss: 0.2356, Accuracy: 0.7983\n",
      "Epoch: 22/50, Train Loss: 0.2931, Test Loss: 0.2328, Accuracy: 0.8020\n",
      "Epoch: 23/50, Train Loss: 0.2918, Test Loss: 0.2285, Accuracy: 0.8135\n",
      "Epoch: 24/50, Train Loss: 0.2918, Test Loss: 0.2301, Accuracy: 0.7950\n",
      "Epoch: 25/50, Train Loss: 0.2920, Test Loss: 0.2151, Accuracy: 0.8270\n",
      "Epoch: 26/50, Train Loss: 0.2926, Test Loss: 0.2160, Accuracy: 0.8250\n",
      "Epoch: 27/50, Train Loss: 0.2897, Test Loss: 0.2141, Accuracy: 0.7837\n",
      "Epoch: 28/50, Train Loss: 0.2925, Test Loss: 0.2114, Accuracy: 0.7983\n",
      "Epoch: 29/50, Train Loss: 0.2904, Test Loss: 0.2178, Accuracy: 0.8163\n",
      "Epoch: 30/50, Train Loss: 0.2916, Test Loss: 0.2140, Accuracy: 0.8355\n",
      "Epoch: 31/50, Train Loss: 0.2899, Test Loss: 0.2031, Accuracy: 0.8330\n",
      "Epoch: 32/50, Train Loss: 0.2880, Test Loss: 0.2059, Accuracy: 0.8237\n",
      "Epoch: 33/50, Train Loss: 0.2892, Test Loss: 0.2051, Accuracy: 0.8293\n",
      "Epoch: 34/50, Train Loss: 0.2901, Test Loss: 0.2113, Accuracy: 0.8247\n",
      "Epoch: 35/50, Train Loss: 0.2897, Test Loss: 0.2102, Accuracy: 0.8290\n",
      "Epoch: 36/50, Train Loss: 0.2893, Test Loss: 0.2059, Accuracy: 0.8385\n",
      "Epoch: 37/50, Train Loss: 0.2892, Test Loss: 0.2134, Accuracy: 0.8175\n",
      "Epoch: 38/50, Train Loss: 0.2901, Test Loss: 0.1947, Accuracy: 0.8295\n",
      "Epoch: 39/50, Train Loss: 0.2892, Test Loss: 0.1952, Accuracy: 0.8187\n",
      "Epoch: 40/50, Train Loss: 0.2875, Test Loss: 0.2155, Accuracy: 0.8173\n",
      "Epoch: 41/50, Train Loss: 0.2889, Test Loss: 0.1989, Accuracy: 0.8297\n",
      "Epoch: 42/50, Train Loss: 0.2879, Test Loss: 0.1967, Accuracy: 0.8263\n",
      "Epoch: 43/50, Train Loss: 0.2879, Test Loss: 0.2010, Accuracy: 0.8220\n",
      "Epoch: 44/50, Train Loss: 0.2873, Test Loss: 0.1848, Accuracy: 0.8277\n",
      "Epoch: 45/50, Train Loss: 0.2871, Test Loss: 0.1974, Accuracy: 0.7995\n",
      "Epoch: 46/50, Train Loss: 0.2873, Test Loss: 0.1892, Accuracy: 0.8290\n",
      "Epoch: 47/50, Train Loss: 0.2885, Test Loss: 0.2007, Accuracy: 0.8225\n",
      "Epoch: 48/50, Train Loss: 0.2862, Test Loss: 0.1853, Accuracy: 0.8240\n",
      "Epoch: 49/50, Train Loss: 0.2876, Test Loss: 0.1898, Accuracy: 0.8180\n",
      "Epoch: 50/50, Train Loss: 0.2877, Test Loss: 0.2076, Accuracy: 0.8080\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for dataset_limit in [20000]:\n",
    "    for batch_size in [32]:\n",
    "        for lr in [0.0005]:\n",
    "            run_test(dataset_limit, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data\n",
      "Read 37842 pairs\n",
      "Running training. dataset_limit=38000. batch_size=32. lr=0.0005\n",
      "Epoch: 1/50, Train Loss: 0.5012, Test Loss: 0.4437, Accuracy: 0.2431\n",
      "Epoch: 2/50, Train Loss: 0.4134, Test Loss: 0.3668, Accuracy: 0.3747\n",
      "Epoch: 3/50, Train Loss: 0.3681, Test Loss: 0.3149, Accuracy: 0.5604\n",
      "Epoch: 4/50, Train Loss: 0.3394, Test Loss: 0.2891, Accuracy: 0.5530\n",
      "Epoch: 5/50, Train Loss: 0.3208, Test Loss: 0.2928, Accuracy: 0.4352\n",
      "Epoch: 6/50, Train Loss: 0.3099, Test Loss: 0.2596, Accuracy: 0.7321\n",
      "Epoch: 7/50, Train Loss: 0.3036, Test Loss: 0.2462, Accuracy: 0.7294\n",
      "Epoch: 8/50, Train Loss: 0.3004, Test Loss: 0.2457, Accuracy: 0.7769\n",
      "Epoch: 9/50, Train Loss: 0.2987, Test Loss: 0.2340, Accuracy: 0.8165\n",
      "Epoch: 10/50, Train Loss: 0.2963, Test Loss: 0.2349, Accuracy: 0.7659\n",
      "Epoch: 11/50, Train Loss: 0.2963, Test Loss: 0.2221, Accuracy: 0.8179\n",
      "Epoch: 12/50, Train Loss: 0.2945, Test Loss: 0.2255, Accuracy: 0.8171\n",
      "Epoch: 13/50, Train Loss: 0.2940, Test Loss: 0.2150, Accuracy: 0.8374\n",
      "Epoch: 14/50, Train Loss: 0.2931, Test Loss: 0.2256, Accuracy: 0.8071\n",
      "Epoch: 15/50, Train Loss: 0.2929, Test Loss: 0.2131, Accuracy: 0.8364\n",
      "Epoch: 16/50, Train Loss: 0.2930, Test Loss: 0.2124, Accuracy: 0.8310\n",
      "Epoch: 17/50, Train Loss: 0.2931, Test Loss: 0.2248, Accuracy: 0.8351\n",
      "Epoch: 18/50, Train Loss: 0.2923, Test Loss: 0.2095, Accuracy: 0.8379\n",
      "Epoch: 19/50, Train Loss: 0.2920, Test Loss: 0.2132, Accuracy: 0.8212\n",
      "Epoch: 20/50, Train Loss: 0.2913, Test Loss: 0.2174, Accuracy: 0.8339\n",
      "Epoch: 21/50, Train Loss: 0.2909, Test Loss: 0.2067, Accuracy: 0.8397\n",
      "Epoch: 22/50, Train Loss: 0.2906, Test Loss: 0.2108, Accuracy: 0.8400\n",
      "Epoch: 23/50, Train Loss: 0.2913, Test Loss: 0.2078, Accuracy: 0.8432\n",
      "Epoch: 24/50, Train Loss: 0.2895, Test Loss: 0.2018, Accuracy: 0.8460\n",
      "Epoch: 25/50, Train Loss: 0.2900, Test Loss: 0.1989, Accuracy: 0.8564\n",
      "Epoch: 26/50, Train Loss: 0.2889, Test Loss: 0.1973, Accuracy: 0.8539\n",
      "Epoch: 27/50, Train Loss: 0.2891, Test Loss: 0.2034, Accuracy: 0.8559\n",
      "Epoch: 28/50, Train Loss: 0.2886, Test Loss: 0.1929, Accuracy: 0.8460\n",
      "Epoch: 29/50, Train Loss: 0.2891, Test Loss: 0.1888, Accuracy: 0.8606\n",
      "Epoch: 30/50, Train Loss: 0.2893, Test Loss: 0.1868, Accuracy: 0.8516\n",
      "Epoch: 31/50, Train Loss: 0.2889, Test Loss: 0.1827, Accuracy: 0.8681\n",
      "Epoch: 32/50, Train Loss: 0.2876, Test Loss: 0.1919, Accuracy: 0.8401\n",
      "Epoch: 33/50, Train Loss: 0.2886, Test Loss: 0.1927, Accuracy: 0.8553\n",
      "Epoch: 34/50, Train Loss: 0.2885, Test Loss: 0.1884, Accuracy: 0.8444\n",
      "Epoch: 35/50, Train Loss: 0.2880, Test Loss: 0.1695, Accuracy: 0.8416\n",
      "Epoch: 36/50, Train Loss: 0.2874, Test Loss: 0.1736, Accuracy: 0.8688\n",
      "Epoch: 37/50, Train Loss: 0.2883, Test Loss: 0.1763, Accuracy: 0.8544\n",
      "Epoch: 38/50, Train Loss: 0.2887, Test Loss: 0.1803, Accuracy: 0.8643\n",
      "Epoch: 39/50, Train Loss: 0.2879, Test Loss: 0.1915, Accuracy: 0.8437\n",
      "Epoch: 40/50, Train Loss: 0.2863, Test Loss: 0.1723, Accuracy: 0.8574\n",
      "Epoch: 41/50, Train Loss: 0.2884, Test Loss: 0.1775, Accuracy: 0.8527\n",
      "Epoch: 42/50, Train Loss: 0.2873, Test Loss: 0.1749, Accuracy: 0.8495\n",
      "Epoch: 43/50, Train Loss: 0.2873, Test Loss: 0.1685, Accuracy: 0.8610\n",
      "Epoch: 44/50, Train Loss: 0.2864, Test Loss: 0.1686, Accuracy: 0.8691\n",
      "Epoch: 45/50, Train Loss: 0.2879, Test Loss: 0.1770, Accuracy: 0.8637\n",
      "Epoch: 46/50, Train Loss: 0.2874, Test Loss: 0.1785, Accuracy: 0.8334\n",
      "Epoch: 47/50, Train Loss: 0.2867, Test Loss: 0.1732, Accuracy: 0.8465\n",
      "Epoch: 48/50, Train Loss: 0.2875, Test Loss: 0.1570, Accuracy: 0.8507\n",
      "Epoch: 49/50, Train Loss: 0.2872, Test Loss: 0.1722, Accuracy: 0.8498\n",
      "Epoch: 50/50, Train Loss: 0.2863, Test Loss: 0.1704, Accuracy: 0.8407\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for dataset_limit in [38000]:\n",
    "    for batch_size in [32]:\n",
    "        for lr in [0.0005]:\n",
    "            run_test(dataset_limit, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data\n",
      "Read 37842 pairs\n",
      "Running training. dataset_limit=38000. batch_size=64. lr=0.001\n",
      "Epoch: 1/50, Train Loss: 0.5041, Test Loss: 0.4310, Accuracy: 0.2039\n",
      "Epoch: 2/50, Train Loss: 0.4153, Test Loss: 0.3797, Accuracy: 0.2957\n",
      "Epoch: 3/50, Train Loss: 0.3725, Test Loss: 0.3221, Accuracy: 0.4995\n",
      "Epoch: 4/50, Train Loss: 0.3447, Test Loss: 0.3053, Accuracy: 0.4459\n",
      "Epoch: 5/50, Train Loss: 0.3262, Test Loss: 0.2771, Accuracy: 0.6594\n",
      "Epoch: 6/50, Train Loss: 0.3135, Test Loss: 0.2652, Accuracy: 0.5657\n",
      "Epoch: 7/50, Train Loss: 0.3063, Test Loss: 0.2480, Accuracy: 0.7573\n",
      "Epoch: 8/50, Train Loss: 0.3011, Test Loss: 0.2541, Accuracy: 0.6977\n",
      "Epoch: 9/50, Train Loss: 0.2990, Test Loss: 0.2461, Accuracy: 0.7561\n",
      "Epoch: 10/50, Train Loss: 0.2988, Test Loss: 0.2514, Accuracy: 0.7280\n",
      "Epoch: 11/50, Train Loss: 0.2972, Test Loss: 0.2399, Accuracy: 0.7853\n",
      "Epoch: 12/50, Train Loss: 0.2964, Test Loss: 0.2524, Accuracy: 0.7551\n",
      "Epoch: 13/50, Train Loss: 0.2977, Test Loss: 0.2396, Accuracy: 0.7696\n",
      "Epoch: 14/50, Train Loss: 0.2949, Test Loss: 0.2472, Accuracy: 0.7893\n",
      "Epoch: 15/50, Train Loss: 0.2944, Test Loss: 0.2410, Accuracy: 0.7868\n",
      "Epoch: 16/50, Train Loss: 0.2945, Test Loss: 0.2212, Accuracy: 0.8195\n",
      "Epoch: 17/50, Train Loss: 0.2929, Test Loss: 0.2200, Accuracy: 0.8206\n",
      "Epoch: 18/50, Train Loss: 0.2925, Test Loss: 0.2395, Accuracy: 0.7710\n",
      "Epoch: 19/50, Train Loss: 0.2925, Test Loss: 0.2378, Accuracy: 0.7601\n",
      "Epoch: 20/50, Train Loss: 0.2925, Test Loss: 0.2172, Accuracy: 0.8275\n",
      "Epoch: 21/50, Train Loss: 0.2913, Test Loss: 0.2133, Accuracy: 0.7864\n",
      "Epoch: 22/50, Train Loss: 0.2924, Test Loss: 0.2362, Accuracy: 0.7639\n",
      "Epoch: 23/50, Train Loss: 0.2919, Test Loss: 0.2197, Accuracy: 0.8331\n",
      "Epoch: 24/50, Train Loss: 0.2906, Test Loss: 0.2109, Accuracy: 0.8364\n",
      "Epoch: 25/50, Train Loss: 0.2920, Test Loss: 0.2256, Accuracy: 0.7952\n",
      "Epoch: 26/50, Train Loss: 0.2910, Test Loss: 0.2031, Accuracy: 0.8409\n",
      "Epoch: 27/50, Train Loss: 0.2903, Test Loss: 0.2087, Accuracy: 0.8327\n",
      "Epoch: 28/50, Train Loss: 0.2910, Test Loss: 0.1860, Accuracy: 0.8510\n",
      "Epoch: 29/50, Train Loss: 0.2907, Test Loss: 0.2166, Accuracy: 0.8079\n",
      "Epoch: 30/50, Train Loss: 0.2901, Test Loss: 0.2073, Accuracy: 0.8339\n",
      "Epoch: 31/50, Train Loss: 0.2898, Test Loss: 0.2118, Accuracy: 0.8245\n",
      "Epoch: 32/50, Train Loss: 0.2890, Test Loss: 0.1988, Accuracy: 0.8548\n",
      "Epoch: 33/50, Train Loss: 0.2923, Test Loss: 0.1965, Accuracy: 0.8495\n",
      "Epoch: 34/50, Train Loss: 0.2900, Test Loss: 0.1929, Accuracy: 0.8360\n",
      "Epoch: 35/50, Train Loss: 0.2888, Test Loss: 0.1869, Accuracy: 0.8548\n",
      "Epoch: 36/50, Train Loss: 0.2894, Test Loss: 0.1917, Accuracy: 0.8580\n",
      "Epoch: 37/50, Train Loss: 0.2891, Test Loss: 0.1980, Accuracy: 0.8444\n",
      "Epoch: 38/50, Train Loss: 0.2894, Test Loss: 0.1946, Accuracy: 0.8555\n",
      "Epoch: 39/50, Train Loss: 0.2877, Test Loss: 0.1878, Accuracy: 0.8531\n",
      "Epoch: 40/50, Train Loss: 0.2892, Test Loss: 0.1945, Accuracy: 0.8502\n",
      "Epoch: 41/50, Train Loss: 0.2877, Test Loss: 0.1745, Accuracy: 0.8522\n",
      "Epoch: 42/50, Train Loss: 0.2885, Test Loss: 0.1733, Accuracy: 0.8585\n",
      "Epoch: 43/50, Train Loss: 0.2884, Test Loss: 0.1892, Accuracy: 0.8282\n",
      "Epoch: 44/50, Train Loss: 0.2892, Test Loss: 0.1855, Accuracy: 0.8569\n",
      "Epoch: 45/50, Train Loss: 0.2870, Test Loss: 0.1818, Accuracy: 0.8374\n",
      "Epoch: 46/50, Train Loss: 0.2886, Test Loss: 0.1869, Accuracy: 0.8301\n",
      "Epoch: 47/50, Train Loss: 0.2888, Test Loss: 0.1795, Accuracy: 0.8507\n",
      "Epoch: 48/50, Train Loss: 0.2869, Test Loss: 0.1677, Accuracy: 0.8485\n",
      "Epoch: 49/50, Train Loss: 0.2890, Test Loss: 0.1833, Accuracy: 0.8272\n",
      "Epoch: 50/50, Train Loss: 0.2899, Test Loss: 0.1759, Accuracy: 0.8530\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for dataset_limit in [38000]:\n",
    "    for batch_size in [64]:\n",
    "        for lr in [0.001]:\n",
    "            run_test(dataset_limit, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data\n",
      "Read 37842 pairs\n",
      "Running training. dataset_limit=38000. batch_size=64. lr=0.0005\n",
      "Epoch: 1/50, Train Loss: 0.5440, Test Loss: 0.4551, Accuracy: 0.2323\n",
      "Epoch: 2/50, Train Loss: 0.4545, Test Loss: 0.4142, Accuracy: 0.3448\n",
      "Epoch: 3/50, Train Loss: 0.4190, Test Loss: 0.3561, Accuracy: 0.3356\n",
      "Epoch: 4/50, Train Loss: 0.3911, Test Loss: 0.3113, Accuracy: 0.6564\n",
      "Epoch: 5/50, Train Loss: 0.3674, Test Loss: 0.3113, Accuracy: 0.4966\n",
      "Epoch: 6/50, Train Loss: 0.3505, Test Loss: 0.3048, Accuracy: 0.4768\n",
      "Epoch: 7/50, Train Loss: 0.3342, Test Loss: 0.2841, Accuracy: 0.5231\n",
      "Epoch: 8/50, Train Loss: 0.3259, Test Loss: 0.2680, Accuracy: 0.5737\n",
      "Epoch: 9/50, Train Loss: 0.3151, Test Loss: 0.2549, Accuracy: 0.6994\n",
      "Epoch: 10/50, Train Loss: 0.3110, Test Loss: 0.2476, Accuracy: 0.6648\n",
      "Epoch: 11/50, Train Loss: 0.3036, Test Loss: 0.2221, Accuracy: 0.8070\n",
      "Epoch: 12/50, Train Loss: 0.3004, Test Loss: 0.2185, Accuracy: 0.8132\n",
      "Epoch: 13/50, Train Loss: 0.2975, Test Loss: 0.2290, Accuracy: 0.7968\n",
      "Epoch: 14/50, Train Loss: 0.2967, Test Loss: 0.2193, Accuracy: 0.8331\n",
      "Epoch: 15/50, Train Loss: 0.2951, Test Loss: 0.2188, Accuracy: 0.8294\n",
      "Epoch: 16/50, Train Loss: 0.2925, Test Loss: 0.2352, Accuracy: 0.7477\n",
      "Epoch: 17/50, Train Loss: 0.2923, Test Loss: 0.2176, Accuracy: 0.8072\n",
      "Epoch: 18/50, Train Loss: 0.2927, Test Loss: 0.2095, Accuracy: 0.8355\n",
      "Epoch: 19/50, Train Loss: 0.2925, Test Loss: 0.2216, Accuracy: 0.8187\n",
      "Epoch: 20/50, Train Loss: 0.2905, Test Loss: 0.2185, Accuracy: 0.8037\n",
      "Epoch: 21/50, Train Loss: 0.2914, Test Loss: 0.2202, Accuracy: 0.8313\n",
      "Epoch: 22/50, Train Loss: 0.2906, Test Loss: 0.2025, Accuracy: 0.8423\n",
      "Epoch: 23/50, Train Loss: 0.2895, Test Loss: 0.2072, Accuracy: 0.8633\n",
      "Epoch: 24/50, Train Loss: 0.2903, Test Loss: 0.2092, Accuracy: 0.8482\n",
      "Epoch: 25/50, Train Loss: 0.2896, Test Loss: 0.2090, Accuracy: 0.8279\n",
      "Epoch: 26/50, Train Loss: 0.2896, Test Loss: 0.1923, Accuracy: 0.8527\n",
      "Epoch: 27/50, Train Loss: 0.2886, Test Loss: 0.2106, Accuracy: 0.8219\n",
      "Epoch: 28/50, Train Loss: 0.2890, Test Loss: 0.1991, Accuracy: 0.8403\n",
      "Epoch: 29/50, Train Loss: 0.2911, Test Loss: 0.1989, Accuracy: 0.8465\n",
      "Epoch: 30/50, Train Loss: 0.2895, Test Loss: 0.1889, Accuracy: 0.8611\n",
      "Epoch: 31/50, Train Loss: 0.2885, Test Loss: 0.1904, Accuracy: 0.8629\n",
      "Epoch: 32/50, Train Loss: 0.2870, Test Loss: 0.1824, Accuracy: 0.8483\n",
      "Epoch: 33/50, Train Loss: 0.2876, Test Loss: 0.2017, Accuracy: 0.8086\n",
      "Epoch: 34/50, Train Loss: 0.2891, Test Loss: 0.1826, Accuracy: 0.8539\n",
      "Epoch: 35/50, Train Loss: 0.2883, Test Loss: 0.1712, Accuracy: 0.8705\n",
      "Epoch: 36/50, Train Loss: 0.2885, Test Loss: 0.1751, Accuracy: 0.8560\n",
      "Epoch: 37/50, Train Loss: 0.2868, Test Loss: 0.1767, Accuracy: 0.8654\n",
      "Epoch: 38/50, Train Loss: 0.2871, Test Loss: 0.1845, Accuracy: 0.8433\n",
      "Epoch: 39/50, Train Loss: 0.2876, Test Loss: 0.1775, Accuracy: 0.8543\n",
      "Epoch: 40/50, Train Loss: 0.2874, Test Loss: 0.1773, Accuracy: 0.8530\n",
      "Epoch: 41/50, Train Loss: 0.2870, Test Loss: 0.2000, Accuracy: 0.8404\n",
      "Epoch: 42/50, Train Loss: 0.2865, Test Loss: 0.1712, Accuracy: 0.8647\n",
      "Epoch: 43/50, Train Loss: 0.2859, Test Loss: 0.1650, Accuracy: 0.8483\n",
      "Epoch: 44/50, Train Loss: 0.2873, Test Loss: 0.1749, Accuracy: 0.8736\n",
      "Epoch: 45/50, Train Loss: 0.2881, Test Loss: 0.1770, Accuracy: 0.8548\n",
      "Epoch: 46/50, Train Loss: 0.2861, Test Loss: 0.1763, Accuracy: 0.8593\n",
      "Epoch: 47/50, Train Loss: 0.2873, Test Loss: 0.1644, Accuracy: 0.8625\n",
      "Epoch: 48/50, Train Loss: 0.2875, Test Loss: 0.1749, Accuracy: 0.8430\n",
      "Epoch: 49/50, Train Loss: 0.2866, Test Loss: 0.1793, Accuracy: 0.8208\n",
      "Epoch: 50/50, Train Loss: 0.2866, Test Loss: 0.1759, Accuracy: 0.8498\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for dataset_limit in [38000]:\n",
    "    for batch_size in [64]:\n",
    "        for lr in [0.0005]:\n",
    "            run_test(dataset_limit, batch_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset_limit = 20_000  batch_size = 16    lr = 0.0005         \n",
    "*Epoch [16/50]. Train loss: 0.6332. Test Loss: 0.9068. Accuracy 0.7007. F1: 0.7027.  \n",
    "\n",
    "dataset_limit = 20_000  batch_size = 32    lr = 0.0005          \n",
    "Epoch [16/50]. Train loss: 0.4637. Test Loss: 0.7815. Accuracy 0.7927. F1: 0.7931.  \n",
    "\n",
    "dataset_limit = 20_000  batch_size = 64    lr = 0.0005          \n",
    "Epoch [26/50]. Train loss: 0.5147. Test Loss: 0.8011. Accuracy 0.7688. F1: 0.7694."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Verifying Client ID with Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # module for os operating, files\n",
    "import random  # used for randomness in the code\n",
    "import torchaudio  # deals with audio data\n",
    "import torchaudio.transforms as T  # for applying audio tranformations\n",
    "from sklearn.model_selection import train_test_split  # splits data\n",
    "\n",
    "import torch  # core pytorch library\n",
    "import torch.optim as optim  # used for updating model weights during training, optimisation algorithms\n",
    "import torch.nn as nn  # classes and tools for neural networks\n",
    "import torch.nn.functional as F  # function for operation of nn like relu, sigmoid\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    ")  # load data in batches for traning and creates datasets\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_positive_triples(\n",
    "    user_to_audio, limit\n",
    "):  # new function that takes dict and limit number of rows for matching samples\n",
    "    # construct queue of user samples\n",
    "    triples = []  # empty list to store all pairs\n",
    "    for user, audios in user_to_audio.items():  # loop through each user's audio samples\n",
    "        user_triples = []  # list to hold pairs for current user in the loop\n",
    "        for i in range(len(audios)):  # loop through each audio sample\n",
    "            for j in range(i + 1, len(audios)):  # loop thorugh next audio samples\n",
    "                if len(user_triples) < 5:  # set limit to 5 pairs per users\n",
    "                    user_triples.append(\n",
    "                        (audios[i], audios[j], torch.tensor([1.0], dtype=torch.float32))\n",
    "                    )  # add to list and put label 1\n",
    "                else:  # ?if list is full\n",
    "                    break  # ?break the cycle\n",
    "            if len(user_triples) >= limit:  # ?\n",
    "                break\n",
    "        triples.extend(user_triples)  # add user pairs to the main list\n",
    "\n",
    "    random.Random(42).shuffle(triples)  # randomly shuffle all paris\n",
    "\n",
    "    return triples[:limit]  # ?return limited number of pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_negative_triples(user_to_audio, limit):  # set fnction to create negative pairs\n",
    "    neg_triples = []  # empty list to sore\n",
    "\n",
    "    # construct queue of user samples\n",
    "    index_user_sample = []  # to hold user pairs\n",
    "    for user, audios in user_to_audio.items():  # loop through users\n",
    "        for audio in audios:  # loop through audios\n",
    "            index_user_sample.append((user, audio))  # store pairs in the list\n",
    "\n",
    "    queue1 = list(index_user_sample)  # copy user audio list\n",
    "    random.Random(42).shuffle(queue1)  # randomly shuffle new list\n",
    "\n",
    "    queue2 = list(index_user_sample)  # another copy of user audio list\n",
    "    random.Random(24).shuffle(queue2)  # randomly shuffle new list\n",
    "\n",
    "    for i in range(len(queue1)):  # loop through queue lists\n",
    "        if queue1[i][0] != queue2[i][0]:  # check if users are same\n",
    "            neg_triples.append(\n",
    "                (queue1[i][1], queue2[i][1], torch.tensor([0.0], dtype=torch.float32))\n",
    "            )  # add a pair if they are not same and set label 0\n",
    "\n",
    "            if len(neg_triples) == limit:  # break if list hit limit\n",
    "                break\n",
    "\n",
    "    return neg_triples  # return list with negative pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_folder(path, limit):  # create new function with two arguments\n",
    "    result = {}  # empty dictionary\n",
    "\n",
    "    for filename in os.listdir(\n",
    "        path\n",
    "    ):  # start loop for all files in the path, lists all files\n",
    "        if filename == \".DS_Store\":  # skips system file\n",
    "            continue\n",
    "        file_path = os.path.join(\n",
    "            path, filename\n",
    "        )  # creates new var of path and name of the file\n",
    "        # print(f\"Processing file: {file_path}\") #used for checking the process\n",
    "\n",
    "        waveform, original_sample_rate = torchaudio.load(\n",
    "            file_path\n",
    "        )  # loads audio data in waveform and sample rate\n",
    "        resampler = T.Resample(\n",
    "            orig_freq=original_sample_rate, new_freq=8000\n",
    "        )  # used to convert all sample rates from orig to 8000\n",
    "        waveform_resampled = resampler(waveform)  # standardize all to 8000\n",
    "\n",
    "        # Ensure waveform is exactly 8000 samples long\n",
    "        if waveform_resampled.shape[1] > 8000:\n",
    "            waveform_resampled = waveform_resampled[\n",
    "                :, :8000\n",
    "            ]  # change to 8000 again if not\n",
    "        elif waveform_resampled.shape[1] < 8000:\n",
    "            padding_size = 8000 - waveform_resampled.shape[1]\n",
    "            waveform_resampled = F.pad(\n",
    "                waveform_resampled, (0, padding_size)\n",
    "            )  # if its shorter it fills with 0 to reach 8000\n",
    "\n",
    "        username = filename.split(\"_\")[0]  # extract userid from file name\n",
    "        if username in result:\n",
    "            result[username].append(\n",
    "                waveform_resampled\n",
    "            )  # if user exist in dictionary, add file to it\n",
    "        else:\n",
    "            result[username] = [waveform_resampled]  # add user and file into dict\n",
    "        if len(result) >= limit:  # stop if dict hits the limit per user\n",
    "            break\n",
    "\n",
    "    positive_triples = to_positive_triples(result, limit=int(limit * 0.5))\n",
    "    negative_triples = to_negative_triples(result, limit=int(limit * 0.5))\n",
    "    return positive_triples, negative_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_triples(path, limit):\n",
    "    digit_limit = limit // 10\n",
    "    digits = [\n",
    "        \"zero\",\n",
    "        \"one\",\n",
    "        \"two\",\n",
    "        \"three\",\n",
    "        \"four\",\n",
    "        \"five\",\n",
    "        \"six\",\n",
    "        \"seven\",\n",
    "        \"eight\",\n",
    "        \"nine\",\n",
    "    ]\n",
    "\n",
    "    all_positive_triples = []\n",
    "    all_negative_triples = []\n",
    "\n",
    "    for digit in digits:\n",
    "        positive_triples, negative_triples = process_audio_folder(\n",
    "            f\"{path}/{digit}\", limit=digit_limit\n",
    "        )\n",
    "        all_positive_triples.extend(positive_triples)\n",
    "        all_negative_triples.extend(negative_triples)\n",
    "\n",
    "    return all_positive_triples, all_negative_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, labels):\n",
    "    true_positives = (\n",
    "        torch.logical_and(predictions >= 0.5, labels == 1).sum().item()\n",
    "    )  # calculate true positives\n",
    "    false_positives = (\n",
    "        torch.logical_and(predictions >= 0.5, labels == 0).sum().item()\n",
    "    )  # calculate false positives\n",
    "    true_negatives = (\n",
    "        torch.logical_and(predictions < 0.5, labels == 0).sum().item()\n",
    "    )  # calculate true negatives\n",
    "    false_negatives = (\n",
    "        torch.logical_and(predictions < 0.5, labels == 1).sum().item()\n",
    "    )  # calculate false negatives\n",
    "\n",
    "    # calculate precision: the ratio of true positives to the total predicted positives\n",
    "    precision = (\n",
    "        true_positives / (true_positives + false_positives)\n",
    "        if true_positives + false_positives > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # calculate recall: the ratio of true positives to the total actual positives\n",
    "    recall = (\n",
    "        true_positives / (true_positives + false_negatives)\n",
    "        if true_positives + false_negatives > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # calculate F1 score: the harmonic mean of precision and recall\n",
    "    f1_score = (\n",
    "        2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    )\n",
    "\n",
    "    # calculate accuracy: the ratio of correct predictions (true positives + true negatives) to the total predictions\n",
    "    accuracy = (\n",
    "        (true_positives + true_negatives)\n",
    "        / (true_positives + false_positives + true_negatives + false_negatives)\n",
    "        if true_positives + false_positives + true_negatives + false_negatives > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # return results\n",
    "    return precision, recall, f1_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(limit: int, cache_dir: str = None):\n",
    "    cache_file = None\n",
    "    if cache_dir:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        cache_file = os.path.join(cache_dir, f\"data_limit_{limit}.pkl\")\n",
    "\n",
    "    if cache_file and os.path.exists(cache_file):\n",
    "        print(f\"Loading data from cache: {cache_file}\")\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            train, test = pickle.load(f)\n",
    "    else:\n",
    "        # Prepare data\n",
    "        print(\"Preparing data\")\n",
    "        positive_triples, negative_triples = to_triples(\n",
    "            \"/root/learning-nn/resources/speech_commands/\", limit\n",
    "        )\n",
    "\n",
    "        print(f\"Read {len(positive_triples)} positive_pairs\")\n",
    "        print(f\"Read {len(negative_triples)} negative_pairs\")\n",
    "\n",
    "        p_train, p_test = train_test_split(\n",
    "            positive_triples, test_size=0.2, random_state=42\n",
    "        )\n",
    "        n_train, n_test = train_test_split(\n",
    "            negative_triples, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        train = p_train + n_train\n",
    "        test = p_test + n_test\n",
    "        random.Random(42).shuffle(train)\n",
    "        random.Random(42).shuffle(test)\n",
    "\n",
    "        if cache_file:\n",
    "            print(f\"Saving data to cache: {cache_file}\")\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump((train, test), f)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, list_triple):\n",
    "        self.list_triple = list_triple\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.list_triple[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.list_triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "CNN Block Architecture\n",
    "\n",
    "[B, 1, 8000] (in_channels, out_channels, kernel=3, stride=1, padding=1)\n",
    "\n",
    "(1, 4, stride=5)\n",
    "\n",
    "1. Conv1 (in_channels, out_channels, kernel_size=kernel, stride=1, padding=padding) [B, 4, 8000]\n",
    "2. BatchNorm1 (out_channels) [B, 4, 8000]\n",
    "3. Relu1 [B, 4, 8000]\n",
    "\n",
    "4. Conv2 (out_channels, out_channels, kernel_size=kernel, stride=1, padding=padding) [B, 4, 8000]\n",
    "5. BatchNorm2 (out_channels) [B, 4, 8000]\n",
    "\n",
    "6. x + shortcut(audio) [B, 4, 8000] + [B, 4, 8000] = [B, 4, 8000]\n",
    "    Conv1(in_channels, out_channels, kernel_size=1, stride=1) [B, 4, 8000]\n",
    "    BatchNorm(out_channels) [B, 4, 8000]\n",
    "\n",
    "7. ReLu [B, 4, 8000]\n",
    "8. Pooling MaxPool1d(kernel_size=kernel, stride=stride, padding=padding) [B, 4, 1600]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut_batchnorm = nn.BatchNorm1d(out_channels)\n",
    "        self.shortcut_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.maxpooling = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, audio):\n",
    "        x = self.conv1(audio)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "\n",
    "        x = x + self.shortcut_batchnorm(self.shortcut_conv(audio))\n",
    "\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpooling(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Input: audio1 [B, 1, 8000], audio2 [B, 1, 8000]\n",
    "\n",
    "1. CNN Block1 (1, 4, stride=5) > [B, 4, 1600]\n",
    "2. CNN Block2 (4, 8, stride=5) > [B, 8, 320]\n",
    "3. CNN Block3 (8, 16, stride=5) > [B, 16, 64]\n",
    "4. CNN Block 4 (16, 32, stride=4) > [B, 32, 16]\n",
    "\n",
    "5. Flatten both tensors, concat [B, 32 * 16 * 2]\n",
    "\n",
    "6. FC layer (32 * 16 * 2, 16) [B, 16]\n",
    "7. Relu [B, 16]\n",
    "8. Dropout (p=0.5) [B, 16]\n",
    "9. FC Layer (16, 1) [B, 1]\n",
    "10. Sigmoid [B, 1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn1 = BlockCNN(in_channels=1, out_channels=4, stride=5)\n",
    "        self.cnn2 = BlockCNN(in_channels=4, out_channels=8, stride=5)\n",
    "        self.cnn3 = BlockCNN(in_channels=8, out_channels=16, stride=5)\n",
    "        self.cnn4 = BlockCNN(in_channels=16, out_channels=32, stride=4)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * 16 * 2, out_features=16)\n",
    "        self.fc2 = nn.Linear(in_features=16, out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout1d(p=0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, audio1, audio2):\n",
    "        x1 = self.cnn1(audio1)\n",
    "        x1 = self.cnn2(x1)\n",
    "        x1 = self.cnn3(x1)\n",
    "        x1 = self.cnn4(x1)\n",
    "\n",
    "        x2 = self.cnn1(audio2)\n",
    "        x2 = self.cnn2(x2)\n",
    "        x2 = self.cnn3(x2)\n",
    "        x2 = self.cnn4(x2)\n",
    "\n",
    "        x1 = torch.flatten(x1, start_dim=1)\n",
    "        x2 = torch.flatten(x2, start_dim=1)\n",
    "\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for audio1, audio2, label in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            audio1 = audio1.to(DEVICE)\n",
    "            audio2 = audio2.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "\n",
    "            prediction = model(audio1, audio2)\n",
    "            loss = criterion(prediction, label)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss * audio1.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        labels = []\n",
    "\n",
    "        running_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for audio1, audio2, label in test_loader:\n",
    "                audio1 = audio1.to(DEVICE)\n",
    "                audio2 = audio2.to(DEVICE)\n",
    "                label = label.to(DEVICE)\n",
    "\n",
    "                prediction = model(audio1, audio2)\n",
    "                loss = criterion(prediction, label) \n",
    "    \n",
    "                running_loss += loss * audio1.size(dim=0)\n",
    "\n",
    "                predictions.append(prediction)\n",
    "                labels.append(label)\n",
    "\n",
    "        test_loss = running_loss / len(test_loader.dataset)\n",
    "\n",
    "        predictions = torch.cat(predictions) \n",
    "        labels = torch.cat(labels) \n",
    "        precision, recall, f1_score, accuracy = evaluate(predictions, labels)\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(\n",
    "    train,\n",
    "    test,\n",
    "    dataset_limit: int,\n",
    "    batch_size: int,\n",
    "    lr: int,\n",
    "    epochs: int,\n",
    "    weight_decay: float,\n",
    "):\n",
    "    # Setting up the training\n",
    "    print(\n",
    "        f\"Running training. dataset_limit={dataset_limit}. batch_size={batch_size}. lr={lr}. epochs={epochs}. weight_decay={weight_decay}\"\n",
    "    )\n",
    "    model = VoiceModel().to(DEVICE)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=VoiceDataset(train), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=VoiceDataset(test), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(\n",
    "        model, train_loader, test_loader, criterion, optimizer, num_epochs=epochs\n",
    "    )\n",
    "\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache: /root/learning-nn/resources/cache/data_limit_30000.pkl\n",
      "Running training. dataset_limit=30000. batch_size=64. lr=0.001. epochs=50. weight_decay=0.001\n",
      "Epoch: 1/50, Train Loss: 0.6510, Test Loss: 0.5933, Accuracy: 0.7207\n",
      "Epoch: 2/50, Train Loss: 0.6244, Test Loss: 0.5861, Accuracy: 0.7320\n",
      "Epoch: 3/50, Train Loss: 0.6197, Test Loss: 0.5946, Accuracy: 0.7123\n",
      "Epoch: 4/50, Train Loss: 0.6108, Test Loss: 0.5609, Accuracy: 0.7428\n",
      "Epoch: 5/50, Train Loss: 0.6017, Test Loss: 0.5366, Accuracy: 0.7527\n",
      "Epoch: 6/50, Train Loss: 0.6008, Test Loss: 0.5390, Accuracy: 0.7547\n",
      "Epoch: 7/50, Train Loss: 0.5962, Test Loss: 0.5528, Accuracy: 0.7495\n",
      "Epoch: 8/50, Train Loss: 0.5956, Test Loss: 0.5394, Accuracy: 0.7537\n",
      "Epoch: 9/50, Train Loss: 0.5917, Test Loss: 0.5422, Accuracy: 0.7610\n",
      "Epoch: 10/50, Train Loss: 0.5827, Test Loss: 0.5045, Accuracy: 0.7858\n",
      "Epoch: 11/50, Train Loss: 0.5595, Test Loss: 0.4725, Accuracy: 0.8083\n",
      "Epoch: 12/50, Train Loss: 0.5526, Test Loss: 0.4732, Accuracy: 0.8100\n",
      "Epoch: 13/50, Train Loss: 0.5475, Test Loss: 0.4519, Accuracy: 0.8263\n",
      "Epoch: 14/50, Train Loss: 0.5426, Test Loss: 0.4489, Accuracy: 0.8275\n",
      "Epoch: 15/50, Train Loss: 0.5382, Test Loss: 0.4421, Accuracy: 0.8252\n",
      "Epoch: 16/50, Train Loss: 0.5338, Test Loss: 0.4356, Accuracy: 0.8282\n",
      "Epoch: 17/50, Train Loss: 0.5177, Test Loss: 0.4120, Accuracy: 0.8408\n",
      "Epoch: 18/50, Train Loss: 0.5082, Test Loss: 0.3873, Accuracy: 0.8667\n",
      "Epoch: 19/50, Train Loss: 0.5012, Test Loss: 0.3972, Accuracy: 0.8482\n",
      "Epoch: 20/50, Train Loss: 0.4992, Test Loss: 0.4233, Accuracy: 0.8105\n",
      "Epoch: 21/50, Train Loss: 0.5000, Test Loss: 0.4178, Accuracy: 0.8178\n",
      "Epoch: 22/50, Train Loss: 0.4931, Test Loss: 0.4524, Accuracy: 0.7897\n",
      "Epoch: 23/50, Train Loss: 0.4900, Test Loss: 0.3629, Accuracy: 0.8740\n",
      "Epoch: 24/50, Train Loss: 0.4882, Test Loss: 0.3598, Accuracy: 0.8738\n",
      "Epoch: 25/50, Train Loss: 0.4832, Test Loss: 0.3524, Accuracy: 0.8600\n",
      "Epoch: 26/50, Train Loss: 0.4820, Test Loss: 0.4253, Accuracy: 0.8095\n",
      "Epoch: 27/50, Train Loss: 0.4803, Test Loss: 0.3997, Accuracy: 0.8220\n",
      "Epoch: 28/50, Train Loss: 0.4826, Test Loss: 0.3391, Accuracy: 0.8823\n",
      "Epoch: 29/50, Train Loss: 0.4780, Test Loss: 0.3786, Accuracy: 0.8582\n",
      "Epoch: 30/50, Train Loss: 0.4751, Test Loss: 0.4107, Accuracy: 0.8340\n",
      "Epoch: 31/50, Train Loss: 0.4796, Test Loss: 0.3394, Accuracy: 0.8902\n",
      "Epoch: 32/50, Train Loss: 0.4740, Test Loss: 0.3212, Accuracy: 0.8815\n",
      "Epoch: 33/50, Train Loss: 0.4748, Test Loss: 0.3554, Accuracy: 0.8700\n",
      "Epoch: 34/50, Train Loss: 0.4738, Test Loss: 0.5225, Accuracy: 0.7455\n",
      "Epoch: 35/50, Train Loss: 0.4721, Test Loss: 0.3317, Accuracy: 0.8920\n",
      "Epoch: 36/50, Train Loss: 0.4640, Test Loss: 0.4400, Accuracy: 0.7903\n",
      "Epoch: 37/50, Train Loss: 0.4677, Test Loss: 0.3866, Accuracy: 0.8413\n",
      "Epoch: 38/50, Train Loss: 0.4700, Test Loss: 0.3734, Accuracy: 0.8497\n",
      "Epoch: 39/50, Train Loss: 0.4666, Test Loss: 0.3951, Accuracy: 0.8298\n",
      "Epoch: 40/50, Train Loss: 0.4616, Test Loss: 0.3717, Accuracy: 0.8623\n",
      "Epoch: 41/50, Train Loss: 0.4643, Test Loss: 0.3264, Accuracy: 0.8950\n",
      "Epoch: 42/50, Train Loss: 0.4591, Test Loss: 0.3728, Accuracy: 0.8592\n",
      "Epoch: 43/50, Train Loss: 0.4578, Test Loss: 0.3639, Accuracy: 0.8663\n",
      "Epoch: 44/50, Train Loss: 0.4582, Test Loss: 0.3492, Accuracy: 0.8717\n",
      "Epoch: 45/50, Train Loss: 0.4600, Test Loss: 0.3250, Accuracy: 0.8882\n",
      "Epoch: 46/50, Train Loss: 0.4534, Test Loss: 0.3252, Accuracy: 0.8845\n",
      "Epoch: 47/50, Train Loss: 0.4578, Test Loss: 0.3193, Accuracy: 0.8868\n",
      "Epoch: 48/50, Train Loss: 0.4541, Test Loss: 0.3990, Accuracy: 0.8262\n",
      "Epoch: 49/50, Train Loss: 0.4524, Test Loss: 0.3936, Accuracy: 0.8238\n",
      "Epoch: 50/50, Train Loss: 0.4518, Test Loss: 0.3283, Accuracy: 0.8865\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for epochs in [50]:\n",
    "    for dataset_limit in [30000]:\n",
    "        train, test = load_data(\n",
    "            dataset_limit, cache_dir=\"/root/learning-nn/resources/cache\"\n",
    "        )\n",
    "        for lr in [0.001]:\n",
    "            for weight_decay in [0.001]:\n",
    "                for batch_size in [64]:\n",
    "                    run_test(train, test, dataset_limit, batch_size, lr, epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache: /root/learning-nn/resources/cache/data_limit_30000.pkl\n",
      "Running training. dataset_limit=30000. batch_size=64. lr=0.0005. epochs=50. weight_decay=0.001\n",
      "Epoch: 1/50, Train Loss: 0.6459, Test Loss: 0.5943, Accuracy: 0.7147\n",
      "Epoch: 2/50, Train Loss: 0.6223, Test Loss: 0.5939, Accuracy: 0.7082\n",
      "Epoch: 3/50, Train Loss: 0.6155, Test Loss: 0.5808, Accuracy: 0.7157\n",
      "Epoch: 4/50, Train Loss: 0.6057, Test Loss: 0.5709, Accuracy: 0.7298\n",
      "Epoch: 5/50, Train Loss: 0.6047, Test Loss: 0.5544, Accuracy: 0.7317\n",
      "Epoch: 6/50, Train Loss: 0.6015, Test Loss: 0.5560, Accuracy: 0.7260\n",
      "Epoch: 7/50, Train Loss: 0.5902, Test Loss: 0.5460, Accuracy: 0.7345\n",
      "Epoch: 8/50, Train Loss: 0.5666, Test Loss: 0.4723, Accuracy: 0.7975\n",
      "Epoch: 9/50, Train Loss: 0.5432, Test Loss: 0.4563, Accuracy: 0.8075\n",
      "Epoch: 10/50, Train Loss: 0.5335, Test Loss: 0.4396, Accuracy: 0.8318\n",
      "Epoch: 11/50, Train Loss: 0.5278, Test Loss: 0.4241, Accuracy: 0.8405\n",
      "Epoch: 12/50, Train Loss: 0.5140, Test Loss: 0.4049, Accuracy: 0.8435\n",
      "Epoch: 13/50, Train Loss: 0.5117, Test Loss: 0.4399, Accuracy: 0.8110\n",
      "Epoch: 14/50, Train Loss: 0.5079, Test Loss: 0.4075, Accuracy: 0.8400\n",
      "Epoch: 15/50, Train Loss: 0.5055, Test Loss: 0.3965, Accuracy: 0.8520\n",
      "Epoch: 16/50, Train Loss: 0.5044, Test Loss: 0.4385, Accuracy: 0.8098\n",
      "Epoch: 17/50, Train Loss: 0.5016, Test Loss: 0.3925, Accuracy: 0.8577\n",
      "Epoch: 18/50, Train Loss: 0.4989, Test Loss: 0.3875, Accuracy: 0.8477\n",
      "Epoch: 19/50, Train Loss: 0.4971, Test Loss: 0.3892, Accuracy: 0.8528\n",
      "Epoch: 20/50, Train Loss: 0.4943, Test Loss: 0.3715, Accuracy: 0.8537\n",
      "Epoch: 21/50, Train Loss: 0.4942, Test Loss: 0.4370, Accuracy: 0.8078\n",
      "Epoch: 22/50, Train Loss: 0.4914, Test Loss: 0.3790, Accuracy: 0.8563\n",
      "Epoch: 23/50, Train Loss: 0.4799, Test Loss: 0.3642, Accuracy: 0.8580\n",
      "Epoch: 24/50, Train Loss: 0.4845, Test Loss: 0.4275, Accuracy: 0.8025\n",
      "Epoch: 25/50, Train Loss: 0.4773, Test Loss: 0.3676, Accuracy: 0.8437\n",
      "Epoch: 26/50, Train Loss: 0.4812, Test Loss: 0.4034, Accuracy: 0.8302\n",
      "Epoch: 27/50, Train Loss: 0.4784, Test Loss: 0.4091, Accuracy: 0.8263\n",
      "Epoch: 28/50, Train Loss: 0.4743, Test Loss: 0.4059, Accuracy: 0.8252\n",
      "Epoch: 29/50, Train Loss: 0.4691, Test Loss: 0.3666, Accuracy: 0.8447\n",
      "Epoch: 30/50, Train Loss: 0.4727, Test Loss: 0.4067, Accuracy: 0.8255\n",
      "Epoch: 31/50, Train Loss: 0.4675, Test Loss: 0.3589, Accuracy: 0.8592\n",
      "Epoch: 32/50, Train Loss: 0.4686, Test Loss: 0.3686, Accuracy: 0.8478\n",
      "Epoch: 33/50, Train Loss: 0.4659, Test Loss: 0.3701, Accuracy: 0.8528\n",
      "Epoch: 34/50, Train Loss: 0.4625, Test Loss: 0.3762, Accuracy: 0.8360\n",
      "Epoch: 35/50, Train Loss: 0.4614, Test Loss: 0.3787, Accuracy: 0.8350\n",
      "Epoch: 36/50, Train Loss: 0.4604, Test Loss: 0.4266, Accuracy: 0.8105\n",
      "Epoch: 37/50, Train Loss: 0.4528, Test Loss: 0.3580, Accuracy: 0.8558\n",
      "Epoch: 38/50, Train Loss: 0.4530, Test Loss: 0.3552, Accuracy: 0.8488\n",
      "Epoch: 39/50, Train Loss: 0.4547, Test Loss: 0.3392, Accuracy: 0.8620\n",
      "Epoch: 40/50, Train Loss: 0.4516, Test Loss: 0.3619, Accuracy: 0.8467\n",
      "Epoch: 41/50, Train Loss: 0.4540, Test Loss: 0.3403, Accuracy: 0.8595\n",
      "Epoch: 42/50, Train Loss: 0.4453, Test Loss: 0.3446, Accuracy: 0.8580\n",
      "Epoch: 43/50, Train Loss: 0.4500, Test Loss: 0.3705, Accuracy: 0.8475\n",
      "Epoch: 44/50, Train Loss: 0.4534, Test Loss: 0.3571, Accuracy: 0.8495\n",
      "Epoch: 45/50, Train Loss: 0.4451, Test Loss: 0.3419, Accuracy: 0.8587\n",
      "Epoch: 46/50, Train Loss: 0.4406, Test Loss: 0.3790, Accuracy: 0.8302\n",
      "Epoch: 47/50, Train Loss: 0.4437, Test Loss: 0.3408, Accuracy: 0.8615\n",
      "Epoch: 48/50, Train Loss: 0.4371, Test Loss: 0.3393, Accuracy: 0.8597\n",
      "Epoch: 49/50, Train Loss: 0.4361, Test Loss: 0.3462, Accuracy: 0.8542\n",
      "Epoch: 50/50, Train Loss: 0.4378, Test Loss: 0.3589, Accuracy: 0.8455\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for epochs in [50]:\n",
    "    for dataset_limit in [30000]:\n",
    "        train, test = load_data(\n",
    "            dataset_limit, cache_dir=\"/root/learning-nn/resources/cache\"\n",
    "        )\n",
    "        for lr in [0.0005]:\n",
    "            for weight_decay in [0.001]:\n",
    "                for batch_size in [64]:\n",
    "                    run_test(train, test, dataset_limit, batch_size, lr, epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache: /root/learning-nn/resources/cache/data_limit_30000.pkl\n",
      "Running training. dataset_limit=30000. batch_size=64. lr=0.0005. epochs=50. weight_decay=0.0005\n",
      "Epoch: 1/50, Train Loss: 0.6529, Test Loss: 0.5964, Accuracy: 0.7262\n",
      "Epoch: 2/50, Train Loss: 0.6229, Test Loss: 0.5755, Accuracy: 0.7347\n",
      "Epoch: 3/50, Train Loss: 0.6085, Test Loss: 0.5543, Accuracy: 0.7458\n",
      "Epoch: 4/50, Train Loss: 0.5991, Test Loss: 0.5318, Accuracy: 0.7627\n",
      "Epoch: 5/50, Train Loss: 0.5720, Test Loss: 0.5008, Accuracy: 0.7887\n",
      "Epoch: 6/50, Train Loss: 0.5524, Test Loss: 0.4512, Accuracy: 0.8185\n",
      "Epoch: 7/50, Train Loss: 0.5326, Test Loss: 0.4195, Accuracy: 0.8385\n",
      "Epoch: 8/50, Train Loss: 0.5211, Test Loss: 0.4123, Accuracy: 0.8500\n",
      "Epoch: 9/50, Train Loss: 0.5166, Test Loss: 0.3863, Accuracy: 0.8380\n",
      "Epoch: 10/50, Train Loss: 0.5085, Test Loss: 0.3969, Accuracy: 0.8520\n",
      "Epoch: 11/50, Train Loss: 0.5032, Test Loss: 0.4964, Accuracy: 0.7517\n",
      "Epoch: 12/50, Train Loss: 0.4964, Test Loss: 0.3871, Accuracy: 0.8542\n",
      "Epoch: 13/50, Train Loss: 0.4914, Test Loss: 0.3827, Accuracy: 0.8495\n",
      "Epoch: 14/50, Train Loss: 0.4885, Test Loss: 0.4110, Accuracy: 0.8237\n",
      "Epoch: 15/50, Train Loss: 0.4894, Test Loss: 0.4878, Accuracy: 0.7572\n",
      "Epoch: 16/50, Train Loss: 0.4790, Test Loss: 0.3566, Accuracy: 0.8730\n",
      "Epoch: 17/50, Train Loss: 0.4809, Test Loss: 0.3428, Accuracy: 0.8810\n",
      "Epoch: 18/50, Train Loss: 0.4749, Test Loss: 0.3445, Accuracy: 0.8712\n",
      "Epoch: 19/50, Train Loss: 0.4766, Test Loss: 0.3459, Accuracy: 0.8735\n",
      "Epoch: 20/50, Train Loss: 0.4684, Test Loss: 0.3312, Accuracy: 0.8780\n",
      "Epoch: 21/50, Train Loss: 0.4683, Test Loss: 0.3393, Accuracy: 0.8797\n",
      "Epoch: 22/50, Train Loss: 0.4643, Test Loss: 0.3366, Accuracy: 0.8758\n",
      "Epoch: 23/50, Train Loss: 0.4686, Test Loss: 0.3854, Accuracy: 0.8515\n",
      "Epoch: 24/50, Train Loss: 0.4603, Test Loss: 0.3327, Accuracy: 0.8800\n",
      "Epoch: 25/50, Train Loss: 0.4601, Test Loss: 0.3510, Accuracy: 0.8692\n",
      "Epoch: 26/50, Train Loss: 0.4589, Test Loss: 0.3673, Accuracy: 0.8512\n",
      "Epoch: 27/50, Train Loss: 0.4575, Test Loss: 0.4194, Accuracy: 0.8087\n",
      "Epoch: 28/50, Train Loss: 0.4472, Test Loss: 0.3141, Accuracy: 0.8860\n",
      "Epoch: 29/50, Train Loss: 0.4532, Test Loss: 0.3201, Accuracy: 0.8795\n",
      "Epoch: 30/50, Train Loss: 0.4521, Test Loss: 0.3361, Accuracy: 0.8740\n",
      "Epoch: 31/50, Train Loss: 0.4451, Test Loss: 0.4179, Accuracy: 0.8087\n",
      "Epoch: 32/50, Train Loss: 0.4453, Test Loss: 0.3223, Accuracy: 0.8768\n",
      "Epoch: 33/50, Train Loss: 0.4429, Test Loss: 0.3646, Accuracy: 0.8468\n",
      "Epoch: 34/50, Train Loss: 0.4395, Test Loss: 0.4364, Accuracy: 0.8003\n",
      "Epoch: 35/50, Train Loss: 0.4366, Test Loss: 0.3587, Accuracy: 0.8578\n",
      "Epoch: 36/50, Train Loss: 0.4385, Test Loss: 0.3165, Accuracy: 0.8730\n",
      "Epoch: 37/50, Train Loss: 0.4338, Test Loss: 0.3425, Accuracy: 0.8707\n",
      "Epoch: 38/50, Train Loss: 0.4372, Test Loss: 0.3751, Accuracy: 0.8452\n",
      "Epoch: 39/50, Train Loss: 0.4312, Test Loss: 0.3024, Accuracy: 0.8835\n",
      "Epoch: 40/50, Train Loss: 0.4305, Test Loss: 0.3117, Accuracy: 0.8817\n",
      "Epoch: 41/50, Train Loss: 0.4304, Test Loss: 0.3040, Accuracy: 0.8818\n",
      "Epoch: 42/50, Train Loss: 0.4221, Test Loss: 0.3163, Accuracy: 0.8803\n",
      "Epoch: 43/50, Train Loss: 0.4237, Test Loss: 0.2956, Accuracy: 0.8885\n",
      "Epoch: 44/50, Train Loss: 0.4207, Test Loss: 0.3627, Accuracy: 0.8522\n",
      "Epoch: 45/50, Train Loss: 0.4175, Test Loss: 0.3077, Accuracy: 0.8792\n",
      "Epoch: 46/50, Train Loss: 0.4183, Test Loss: 0.3050, Accuracy: 0.8813\n",
      "Epoch: 47/50, Train Loss: 0.4131, Test Loss: 0.2992, Accuracy: 0.8818\n",
      "Epoch: 48/50, Train Loss: 0.4159, Test Loss: 0.3169, Accuracy: 0.8728\n",
      "Epoch: 49/50, Train Loss: 0.4148, Test Loss: 0.3633, Accuracy: 0.8483\n",
      "Epoch: 50/50, Train Loss: 0.4144, Test Loss: 0.3600, Accuracy: 0.8495\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for epochs in [50]:\n",
    "    for dataset_limit in [30000]:\n",
    "        train, test = load_data(\n",
    "            dataset_limit, cache_dir=\"/root/learning-nn/resources/cache\"\n",
    "        )\n",
    "        for lr in [0.0005]:\n",
    "            for weight_decay in [0.0005]:\n",
    "                for batch_size in [64]:\n",
    "                    run_test(train, test, dataset_limit, batch_size, lr, epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache: /root/learning-nn/resources/cache/data_limit_30000.pkl\n",
      "Running training. dataset_limit=30000. batch_size=64. lr=0.001. epochs=50. weight_decay=0.0005\n",
      "Epoch: 1/50, Train Loss: 0.6495, Test Loss: 0.5907, Accuracy: 0.7207\n",
      "Epoch: 2/50, Train Loss: 0.6214, Test Loss: 0.5704, Accuracy: 0.7298\n",
      "Epoch: 3/50, Train Loss: 0.6104, Test Loss: 0.5528, Accuracy: 0.7463\n",
      "Epoch: 4/50, Train Loss: 0.6044, Test Loss: 0.5467, Accuracy: 0.7500\n",
      "Epoch: 5/50, Train Loss: 0.5991, Test Loss: 0.5537, Accuracy: 0.7385\n",
      "Epoch: 6/50, Train Loss: 0.5941, Test Loss: 0.5434, Accuracy: 0.7625\n",
      "Epoch: 7/50, Train Loss: 0.5804, Test Loss: 0.4854, Accuracy: 0.8010\n",
      "Epoch: 8/50, Train Loss: 0.5597, Test Loss: 0.4643, Accuracy: 0.8110\n",
      "Epoch: 9/50, Train Loss: 0.5513, Test Loss: 0.4659, Accuracy: 0.8158\n",
      "Epoch: 10/50, Train Loss: 0.5372, Test Loss: 0.4564, Accuracy: 0.8262\n",
      "Epoch: 11/50, Train Loss: 0.5410, Test Loss: 0.4478, Accuracy: 0.8215\n",
      "Epoch: 12/50, Train Loss: 0.5344, Test Loss: 0.4354, Accuracy: 0.8243\n",
      "Epoch: 13/50, Train Loss: 0.5262, Test Loss: 0.4359, Accuracy: 0.8255\n",
      "Epoch: 14/50, Train Loss: 0.5317, Test Loss: 0.4408, Accuracy: 0.8270\n",
      "Epoch: 15/50, Train Loss: 0.5261, Test Loss: 0.4339, Accuracy: 0.8310\n",
      "Epoch: 16/50, Train Loss: 0.5213, Test Loss: 0.4245, Accuracy: 0.8375\n",
      "Epoch: 17/50, Train Loss: 0.5139, Test Loss: 0.4145, Accuracy: 0.8330\n",
      "Epoch: 18/50, Train Loss: 0.5132, Test Loss: 0.4382, Accuracy: 0.8212\n",
      "Epoch: 19/50, Train Loss: 0.4988, Test Loss: 0.3813, Accuracy: 0.8585\n",
      "Epoch: 20/50, Train Loss: 0.5020, Test Loss: 0.3832, Accuracy: 0.8455\n",
      "Epoch: 21/50, Train Loss: 0.4956, Test Loss: 0.3734, Accuracy: 0.8538\n",
      "Epoch: 22/50, Train Loss: 0.4955, Test Loss: 0.3764, Accuracy: 0.8663\n",
      "Epoch: 23/50, Train Loss: 0.4887, Test Loss: 0.4116, Accuracy: 0.8435\n",
      "Epoch: 24/50, Train Loss: 0.4835, Test Loss: 0.3778, Accuracy: 0.8603\n",
      "Epoch: 25/50, Train Loss: 0.4775, Test Loss: 0.4152, Accuracy: 0.8135\n",
      "Epoch: 26/50, Train Loss: 0.4778, Test Loss: 0.5268, Accuracy: 0.7355\n",
      "Epoch: 27/50, Train Loss: 0.4802, Test Loss: 0.3620, Accuracy: 0.8632\n",
      "Epoch: 28/50, Train Loss: 0.4696, Test Loss: 0.3317, Accuracy: 0.8805\n",
      "Epoch: 29/50, Train Loss: 0.4735, Test Loss: 0.3686, Accuracy: 0.8733\n",
      "Epoch: 30/50, Train Loss: 0.4729, Test Loss: 0.3395, Accuracy: 0.8692\n",
      "Epoch: 31/50, Train Loss: 0.4706, Test Loss: 0.3631, Accuracy: 0.8583\n",
      "Epoch: 32/50, Train Loss: 0.4654, Test Loss: 0.3777, Accuracy: 0.8505\n",
      "Epoch: 33/50, Train Loss: 0.4655, Test Loss: 0.3505, Accuracy: 0.8610\n",
      "Epoch: 34/50, Train Loss: 0.4689, Test Loss: 0.3589, Accuracy: 0.8673\n",
      "Epoch: 35/50, Train Loss: 0.4651, Test Loss: 0.3973, Accuracy: 0.8352\n",
      "Epoch: 36/50, Train Loss: 0.4638, Test Loss: 0.3347, Accuracy: 0.8763\n",
      "Epoch: 37/50, Train Loss: 0.4641, Test Loss: 0.3177, Accuracy: 0.8740\n",
      "Epoch: 38/50, Train Loss: 0.4588, Test Loss: 0.3345, Accuracy: 0.8697\n",
      "Epoch: 39/50, Train Loss: 0.4572, Test Loss: 0.3582, Accuracy: 0.8570\n",
      "Epoch: 40/50, Train Loss: 0.4552, Test Loss: 0.3628, Accuracy: 0.8570\n",
      "Epoch: 41/50, Train Loss: 0.4532, Test Loss: 0.3202, Accuracy: 0.8807\n",
      "Epoch: 42/50, Train Loss: 0.4523, Test Loss: 0.3228, Accuracy: 0.8812\n",
      "Epoch: 43/50, Train Loss: 0.4499, Test Loss: 0.3206, Accuracy: 0.8808\n",
      "Epoch: 44/50, Train Loss: 0.4484, Test Loss: 0.3244, Accuracy: 0.8765\n",
      "Epoch: 45/50, Train Loss: 0.4474, Test Loss: 0.4507, Accuracy: 0.7927\n",
      "Epoch: 46/50, Train Loss: 0.4440, Test Loss: 0.3434, Accuracy: 0.8573\n",
      "Epoch: 47/50, Train Loss: 0.4379, Test Loss: 0.3466, Accuracy: 0.8705\n",
      "Epoch: 48/50, Train Loss: 0.4438, Test Loss: 0.3437, Accuracy: 0.8733\n",
      "Epoch: 49/50, Train Loss: 0.4385, Test Loss: 0.3002, Accuracy: 0.8838\n",
      "Epoch: 50/50, Train Loss: 0.4391, Test Loss: 0.3363, Accuracy: 0.8647\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for epochs in [50]:\n",
    "    for dataset_limit in [30000]:\n",
    "        train, test = load_data(\n",
    "            dataset_limit, cache_dir=\"/root/learning-nn/resources/cache\"\n",
    "        )\n",
    "        for lr in [0.001]:\n",
    "            for weight_decay in [0.0005]:\n",
    "                for batch_size in [64]:\n",
    "                    run_test(train, test, dataset_limit, batch_size, lr, epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache: /root/learning-nn/resources/cache/data_limit_40000.pkl\n",
      "Running training. dataset_limit=40000. batch_size=64. lr=0.0005. epochs=50. weight_decay=0.0005\n",
      "Epoch: 1/50, Train Loss: 0.6494, Test Loss: 0.5965, Accuracy: 0.6960\n",
      "Epoch: 2/50, Train Loss: 0.6244, Test Loss: 0.5755, Accuracy: 0.7214\n",
      "Epoch: 3/50, Train Loss: 0.6091, Test Loss: 0.5503, Accuracy: 0.7496\n",
      "Epoch: 4/50, Train Loss: 0.6017, Test Loss: 0.5349, Accuracy: 0.7601\n",
      "Epoch: 5/50, Train Loss: 0.5924, Test Loss: 0.5310, Accuracy: 0.7715\n",
      "Epoch: 6/50, Train Loss: 0.5743, Test Loss: 0.4803, Accuracy: 0.8136\n",
      "Epoch: 7/50, Train Loss: 0.5430, Test Loss: 0.4459, Accuracy: 0.8281\n",
      "Epoch: 8/50, Train Loss: 0.5275, Test Loss: 0.4071, Accuracy: 0.8498\n",
      "Epoch: 9/50, Train Loss: 0.5112, Test Loss: 0.3915, Accuracy: 0.8601\n",
      "Epoch: 10/50, Train Loss: 0.5053, Test Loss: 0.3704, Accuracy: 0.8675\n",
      "Epoch: 11/50, Train Loss: 0.4983, Test Loss: 0.3641, Accuracy: 0.8720\n",
      "Epoch: 12/50, Train Loss: 0.4956, Test Loss: 0.3526, Accuracy: 0.8752\n",
      "Epoch: 13/50, Train Loss: 0.4872, Test Loss: 0.3487, Accuracy: 0.8710\n",
      "Epoch: 14/50, Train Loss: 0.4846, Test Loss: 0.3424, Accuracy: 0.8820\n",
      "Epoch: 15/50, Train Loss: 0.4833, Test Loss: 0.3566, Accuracy: 0.8776\n",
      "Epoch: 16/50, Train Loss: 0.4822, Test Loss: 0.3407, Accuracy: 0.8750\n",
      "Epoch: 17/50, Train Loss: 0.4797, Test Loss: 0.3467, Accuracy: 0.8752\n",
      "Epoch: 18/50, Train Loss: 0.4712, Test Loss: 0.3388, Accuracy: 0.8800\n",
      "Epoch: 19/50, Train Loss: 0.4717, Test Loss: 0.3266, Accuracy: 0.8842\n",
      "Epoch: 20/50, Train Loss: 0.4721, Test Loss: 0.3289, Accuracy: 0.8846\n",
      "Epoch: 21/50, Train Loss: 0.4657, Test Loss: 0.3472, Accuracy: 0.8741\n",
      "Epoch: 22/50, Train Loss: 0.4600, Test Loss: 0.3716, Accuracy: 0.8494\n",
      "Epoch: 23/50, Train Loss: 0.4639, Test Loss: 0.3192, Accuracy: 0.8845\n",
      "Epoch: 24/50, Train Loss: 0.4598, Test Loss: 0.3143, Accuracy: 0.8892\n",
      "Epoch: 25/50, Train Loss: 0.4597, Test Loss: 0.3668, Accuracy: 0.8488\n",
      "Epoch: 26/50, Train Loss: 0.4533, Test Loss: 0.4038, Accuracy: 0.8154\n",
      "Epoch: 27/50, Train Loss: 0.4549, Test Loss: 0.3202, Accuracy: 0.8852\n",
      "Epoch: 28/50, Train Loss: 0.4554, Test Loss: 0.3030, Accuracy: 0.8858\n",
      "Epoch: 29/50, Train Loss: 0.4518, Test Loss: 0.3000, Accuracy: 0.8889\n",
      "Epoch: 30/50, Train Loss: 0.4462, Test Loss: 0.3328, Accuracy: 0.8724\n",
      "Epoch: 31/50, Train Loss: 0.4469, Test Loss: 0.3698, Accuracy: 0.8505\n",
      "Epoch: 32/50, Train Loss: 0.4453, Test Loss: 0.2935, Accuracy: 0.8894\n",
      "Epoch: 33/50, Train Loss: 0.4450, Test Loss: 0.3003, Accuracy: 0.8872\n",
      "Epoch: 34/50, Train Loss: 0.4423, Test Loss: 0.3125, Accuracy: 0.8834\n",
      "Epoch: 35/50, Train Loss: 0.4398, Test Loss: 0.3051, Accuracy: 0.8866\n",
      "Epoch: 36/50, Train Loss: 0.4410, Test Loss: 0.3406, Accuracy: 0.8624\n",
      "Epoch: 37/50, Train Loss: 0.4398, Test Loss: 0.3025, Accuracy: 0.8871\n",
      "Epoch: 38/50, Train Loss: 0.4333, Test Loss: 0.3091, Accuracy: 0.8802\n",
      "Epoch: 39/50, Train Loss: 0.4352, Test Loss: 0.2972, Accuracy: 0.8910\n",
      "Epoch: 40/50, Train Loss: 0.4344, Test Loss: 0.2937, Accuracy: 0.8890\n",
      "Epoch: 41/50, Train Loss: 0.4277, Test Loss: 0.3055, Accuracy: 0.8816\n",
      "Epoch: 42/50, Train Loss: 0.4291, Test Loss: 0.3197, Accuracy: 0.8730\n",
      "Epoch: 43/50, Train Loss: 0.4300, Test Loss: 0.3214, Accuracy: 0.8716\n",
      "Epoch: 44/50, Train Loss: 0.4288, Test Loss: 0.2981, Accuracy: 0.8874\n",
      "Epoch: 45/50, Train Loss: 0.4251, Test Loss: 0.3117, Accuracy: 0.8759\n",
      "Epoch: 46/50, Train Loss: 0.4218, Test Loss: 0.3229, Accuracy: 0.8658\n",
      "Epoch: 47/50, Train Loss: 0.4222, Test Loss: 0.2897, Accuracy: 0.8832\n",
      "Epoch: 48/50, Train Loss: 0.4222, Test Loss: 0.3352, Accuracy: 0.8644\n",
      "Epoch: 49/50, Train Loss: 0.4205, Test Loss: 0.2909, Accuracy: 0.8859\n",
      "Epoch: 50/50, Train Loss: 0.4167, Test Loss: 0.3096, Accuracy: 0.8804\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for epochs in [50]:\n",
    "    for dataset_limit in [40000]:\n",
    "        train, test = load_data(\n",
    "            dataset_limit, cache_dir=\"/root/learning-nn/resources/cache\"\n",
    "        )\n",
    "        for lr in [0.0005]:\n",
    "            for weight_decay in [0.0005]:\n",
    "                for batch_size in [64]:\n",
    "                    run_test(train, test, dataset_limit, batch_size, lr, epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data\n",
      "Read 20000 positive_pairs\n",
      "Read 20000 negative_pairs\n",
      "Saving data to cache: /root/learning-nn/resources/cache/data_limit_40000.pkl\n",
      "Running training. dataset_limit=40000. batch_size=128. lr=0.0005. epochs=50. weight_decay=0.00025\n",
      "Epoch: 1/50, Train Loss: 0.6580, Test Loss: 0.5886, Accuracy: 0.7212\n",
      "Epoch: 2/50, Train Loss: 0.6261, Test Loss: 0.5728, Accuracy: 0.7244\n",
      "Epoch: 3/50, Train Loss: 0.6146, Test Loss: 0.5641, Accuracy: 0.7406\n",
      "Epoch: 4/50, Train Loss: 0.6062, Test Loss: 0.5539, Accuracy: 0.7454\n",
      "Epoch: 5/50, Train Loss: 0.6033, Test Loss: 0.5444, Accuracy: 0.7528\n",
      "Epoch: 6/50, Train Loss: 0.6010, Test Loss: 0.5352, Accuracy: 0.7614\n",
      "Epoch: 7/50, Train Loss: 0.5944, Test Loss: 0.5268, Accuracy: 0.7692\n",
      "Epoch: 8/50, Train Loss: 0.5827, Test Loss: 0.4997, Accuracy: 0.8003\n",
      "Epoch: 9/50, Train Loss: 0.5531, Test Loss: 0.4618, Accuracy: 0.8193\n",
      "Epoch: 10/50, Train Loss: 0.5386, Test Loss: 0.4299, Accuracy: 0.8309\n",
      "Epoch: 11/50, Train Loss: 0.5268, Test Loss: 0.4157, Accuracy: 0.8504\n",
      "Epoch: 12/50, Train Loss: 0.5174, Test Loss: 0.4068, Accuracy: 0.8508\n",
      "Epoch: 13/50, Train Loss: 0.5094, Test Loss: 0.3749, Accuracy: 0.8674\n",
      "Epoch: 14/50, Train Loss: 0.4991, Test Loss: 0.3642, Accuracy: 0.8638\n",
      "Epoch: 15/50, Train Loss: 0.4901, Test Loss: 0.3561, Accuracy: 0.8701\n",
      "Epoch: 16/50, Train Loss: 0.4901, Test Loss: 0.3446, Accuracy: 0.8815\n",
      "Epoch: 17/50, Train Loss: 0.4784, Test Loss: 0.3415, Accuracy: 0.8800\n",
      "Epoch: 18/50, Train Loss: 0.4790, Test Loss: 0.3348, Accuracy: 0.8830\n",
      "Epoch: 19/50, Train Loss: 0.4765, Test Loss: 0.3414, Accuracy: 0.8788\n",
      "Epoch: 20/50, Train Loss: 0.4721, Test Loss: 0.3381, Accuracy: 0.8740\n",
      "Epoch: 21/50, Train Loss: 0.4724, Test Loss: 0.3329, Accuracy: 0.8769\n",
      "Epoch: 22/50, Train Loss: 0.4686, Test Loss: 0.3668, Accuracy: 0.8615\n",
      "Epoch: 23/50, Train Loss: 0.4687, Test Loss: 0.3377, Accuracy: 0.8799\n",
      "Epoch: 24/50, Train Loss: 0.4620, Test Loss: 0.3512, Accuracy: 0.8684\n",
      "Epoch: 25/50, Train Loss: 0.4627, Test Loss: 0.3299, Accuracy: 0.8789\n",
      "Epoch: 26/50, Train Loss: 0.4555, Test Loss: 0.3292, Accuracy: 0.8736\n",
      "Epoch: 27/50, Train Loss: 0.4556, Test Loss: 0.3463, Accuracy: 0.8642\n",
      "Epoch: 28/50, Train Loss: 0.4543, Test Loss: 0.3138, Accuracy: 0.8808\n",
      "Epoch: 29/50, Train Loss: 0.4539, Test Loss: 0.3293, Accuracy: 0.8774\n",
      "Epoch: 30/50, Train Loss: 0.4496, Test Loss: 0.3039, Accuracy: 0.8848\n",
      "Epoch: 31/50, Train Loss: 0.4462, Test Loss: 0.3130, Accuracy: 0.8808\n",
      "Epoch: 32/50, Train Loss: 0.4470, Test Loss: 0.3411, Accuracy: 0.8612\n",
      "Epoch: 33/50, Train Loss: 0.4408, Test Loss: 0.3116, Accuracy: 0.8782\n",
      "Epoch: 34/50, Train Loss: 0.4439, Test Loss: 0.3135, Accuracy: 0.8805\n",
      "Epoch: 35/50, Train Loss: 0.4425, Test Loss: 0.3207, Accuracy: 0.8688\n",
      "Epoch: 36/50, Train Loss: 0.4367, Test Loss: 0.2952, Accuracy: 0.8879\n",
      "Epoch: 37/50, Train Loss: 0.4384, Test Loss: 0.3262, Accuracy: 0.8671\n",
      "Epoch: 38/50, Train Loss: 0.4381, Test Loss: 0.3123, Accuracy: 0.8755\n",
      "Epoch: 39/50, Train Loss: 0.4355, Test Loss: 0.2967, Accuracy: 0.8830\n",
      "Epoch: 40/50, Train Loss: 0.4357, Test Loss: 0.3030, Accuracy: 0.8779\n",
      "Epoch: 41/50, Train Loss: 0.4311, Test Loss: 0.3001, Accuracy: 0.8869\n",
      "Epoch: 42/50, Train Loss: 0.4344, Test Loss: 0.3140, Accuracy: 0.8748\n",
      "Epoch: 43/50, Train Loss: 0.4332, Test Loss: 0.2926, Accuracy: 0.8836\n",
      "Epoch: 44/50, Train Loss: 0.4260, Test Loss: 0.2996, Accuracy: 0.8809\n",
      "Epoch: 45/50, Train Loss: 0.4241, Test Loss: 0.2985, Accuracy: 0.8778\n",
      "Epoch: 46/50, Train Loss: 0.4227, Test Loss: 0.2956, Accuracy: 0.8800\n",
      "Epoch: 47/50, Train Loss: 0.4207, Test Loss: 0.2986, Accuracy: 0.8802\n",
      "Epoch: 48/50, Train Loss: 0.4214, Test Loss: 0.3238, Accuracy: 0.8636\n",
      "Epoch: 49/50, Train Loss: 0.4221, Test Loss: 0.3177, Accuracy: 0.8668\n",
      "Epoch: 50/50, Train Loss: 0.4195, Test Loss: 0.3042, Accuracy: 0.8745\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for epochs in [50]:\n",
    "    for dataset_limit in [40000]:\n",
    "        train, test = load_data(\n",
    "            dataset_limit, cache_dir=\"/root/learning-nn/resources/cache\"\n",
    "        )\n",
    "        for lr in [0.0005]:\n",
    "            for weight_decay in [0.00025]:\n",
    "                for batch_size in [128]:\n",
    "                    run_test(train, test, dataset_limit, batch_size, lr, epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Implementing voice authentication is a strategic move for Revolut, offering numerous advantages. From enhanced security and fraud prevention to improved customer trust and operational efficiency, this technology addresses critical business needs while positioning Revolut as a leader in fintech innovation. With the growing adoption of voice biometrics worldwide, now is the ideal time for Revolut to embrace this technology and provide its customers with the most secure and convenient banking experience.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
